{
  "author": "868c6e58-57c1-48ee-8c67-832dbedcac18",
  "retriever": {
    "collection": {
      "name": "mt-rag-ibmcloud-elser-512-100-20240502",
      "size": "72442"
    },
    "parameters": {
      "max_count": 3,
      "max_utterances": -1,
      "query_syntax": "{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"text_expansion\": {\n          \"ml.tokens\": {\n            \"model_id\": \".elser_model_1\",\n            \"model_text\": \"${QUERY}\"\n          }\n        }\n      }\n    }\n  }\n}",
      "project": "{\n  \"text\": \"text\",\n  \"title\": \"title\",\n  \"url\": \"url\"\n}"
    }
  },
  "generator": {
    "id": "mistralai/mixtral-8x7b-instruct-v01",
    "name": "mixtral-8x7b-instruct-v01",
    "prompt": {
      "template": "[INST]\n${CONTEXT}\n${SYSTEM_INST}\n${INPUT}\n[/INST]\nanswer:",
      "input": "${SPEAKER}: ${TEXT}\n",
      "context": "[DOCUMENT]\n${TEXT}\n[END]\n",
      "system_instruction": "You are an AI Assistant, tasked with providing responses that are well-grounded in the provided documents. Given one or more documents and a user query, generate a response to the query. If no answer can be found in the documents, say, \"I do not have specific information\"."
    },
    "parameters": {
      "min_new_tokens": 1,
      "max_new_tokens": 512,
      "repetition_penalty": 1.05,
      "stop_sequences": [
        "<|endoftext|>"
      ]
    }
  },
  "messages": [
    {
      "speaker": "user",
      "text": "What are the steps to be taken to gather the relevant worker node data?",
      "timestamp": 1724165026,
      "enrichments": {
        "Question Type": [
          "How-To"
        ],
        "Multi-Turn": [
          "N/A"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "   There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        ",
      "timestamp": 1724165038,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 17.551182,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724165519
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 16.794611,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724165517
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10534-545406-546780",
          "text": "\n* [If all worker nodes in a single zone, subnet, or VLAN are affected](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-steps-zone)\n* [If all worker nodes in a cluster are affected](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-steps-all)\n* [If worker nodes switch between normal and critical states](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-steps-switch)\n\n\n\n* [Gathering data for a support case](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather)\n\n\n\n* [Before you begin](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather-before)\n* [Gathering data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather-steps)\n\n\n\n\n\n[VPC: Why can't I create worker nodes on dedicated hosts?](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-worker-dedicatedts-worker-dedicated)\n\n[Why doesn't replacing a worker node create a worker node?](https://cloud.ibm.com/docs/openshift?topic=openshift-auto-rebalance-offauto-rebalance-off)\n\n[Classic: Why is the bare metal instance ID inconsistent with worker records?](https://cloud.ibm.com/docs/openshift?topic=openshift-bm_machine_idbm_machine_id)",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-sitemap",
          "score": 16.251226,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724676778
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "To gather the relevant worker node data, follow these steps:\n1. Check the conditions of your worker nodes and cluster before you gather data. Specifically, you should:\n  - Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n2. Gather information about your worker nodes by running the following command:\n  - For IBM Cloud Kubernetes Service: `kubectl top node`\n  - For IBM Cloud Satellite Location (using OpenShift): `oc top node`\n3. After gathering the worker node data, open a support ticket and include the worker node information you gathered.\n4. Before opening the support ticket, review the information and follow any troubleshooting steps in the following documents:\n  - Debugging worker nodes\n  - Worker node states\n  - Troubleshooting worker nodes in Critical or NotReady state\n5. If all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\nReferences:\n- IBM Cloud Kubernetes Service: <https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather>\n- IBM Cloud Satellite Location (using OpenShift): <https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather>"
    },
    {
      "speaker": "user",
      "text": "How can i update a classic worker node?",
      "timestamp": 1724165578,
      "enrichments": {
        "Question Type": [
          "How-To"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.",
      "timestamp": 1724165582,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 52.078983,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724165740
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 50.989445,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724165675
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_06160-12611-14167",
          "text": "\nIf any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2. Make sure that you have [removed any added webhooks](https://cloud.ibm.com/docs/containers?topic=containers-ts-delete-webhooks) from the cluster.\n\n\n\n\n\n\n\n Gathering data \n\nFollow the steps to gather the relevant worker node data.\n\n\n\n1. Get the details of each node. Save the output details to include in your support ticket.\n\nkubectl describe node <node-ip-address>\n2. Run the [Diagnostics and Debug Tool](https://cloud.ibm.com/docs/containers?topic=containers-debug-tool) . Export the kube and network test results to a compressed file and save the file to include in your support ticket. If all workers in your cluster are affected, you can skip this step as the debug tools cannot work properly if all worker nodes are disrupted.\n3. Show that there are no added mutating or validating webhooks remaining in your cluster by getting the webhook details. Save the command output to include in your support ticket. Note that the following mutating webhooks might remain and do not need to be deleted: alertmanagerconfigs.openshift, managed-storage-validation-webhooks, multus.openshift.io, performance-addon-operator, prometheusrules.openshift.io,snapshot.storage.k8s.io.\n\nkubectl get mutatingwebhookconfigurations\n\nkubectl get validatingwebhookconfigurations\n4.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.170155,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724165723
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 26.31584,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724165755
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10642-6354-8294",
          "text": "\nYou notice that an update is available for your worker nodes in a [classic infrastructure](https://cloud.ibm.com/docs/openshift?topic=openshift-infrastructure_providers) cluster. What does that mean? As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version.\n\nApplies to only classic clusters. Have a VPC cluster? See [Updating VPC worker nodes](https://cloud.ibm.com/docs/openshift?topic=openshift-updatevpc_worker_node) instead.\n\nFor the latest security patches and fixes, make sure to update your worker nodes to the latest patch as soon as possible after it is available. For more information about the latest updates, review the [change log](https://cloud.ibm.com/docs/containers?topic=containers-changelog).\n\nTo update Satellite worker nodes, see [Updating hosts that are assigned as worker nodes to Satellite-enabled services](https://cloud.ibm.com/docs/satellite?topic=satellite-host-update-workers).\n\n\n\n* Patch: A worker node patch update includes security fixes. You can update the classic worker node to the latest patch by using the ibmcloud oc worker reload or update commands. Keep in mind that the update command also updates the worker node to the same major.minor version as the master and latest patch version, if a major.minor version update is also available.\n* Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-update",
          "score": 25.768103,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724165764
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_05891-121193-123094",
          "text": "\nClassic infrastructure\n\nUpdate worker nodes to apply the latest security updates and patches to the operating system, and to update the Kubernetes version to match the version of the Kubernetes master. You can update the master Kubernetes version with the ibmcloud ks cluster master update[command](https://cloud.ibm.com/docs/containers?topic=containers-kubernetes-service-clics_cluster_update). Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n\nTo update a worker node in a VPC cluster, use the [ibmcloud ks worker replace command](https://cloud.ibm.com/docs/containers?topic=containers-kubernetes-service-clicli_worker_replace) instead.\n\nRunning ibmcloud ks worker update can cause downtime for your apps and services. During the update, all pods are rescheduled onto other worker nodes, the worker node is reimaged, and data is deleted if not stored outside the pod. To avoid downtime, [ensure that you have enough worker nodes to handle your workload while the selected worker nodes are updating](https://cloud.ibm.com/docs/containers?topic=containers-updateworker_node).\n\nTo update Satellite worker nodes, see [Updating hosts that are assigned as worker nodes to Satellite-enabled services](https://cloud.ibm.com/docs/satellite?topic=satellite-host-update-workers).\n\nYou might need to change your YAML files for deployments before you update. Review this [release note](https://cloud.ibm.com/docs/containers?topic=containers-cs_versions) for details.\n\nibmcloud ks worker update --cluster CLUSTER --worker WORKER_ID [-f] [-q]\n\nMinimum required permissions\n: Operator platform access role for the cluster in IBM Cloud Kubernetes Service\n\nCommand options:\n\n-c, --cluster CLUSTER\n: Required: The name or ID of the cluster where you list available worker nodes.\n\n-w, --worker WORKER",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-kubernetes-service-cli",
          "score": 25.652802,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724165907
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_05603-45298-46323",
          "text": "\nWorker node patch updates can be applied by updating, reloading (in classic infrastructure), or replacing (in VPC infrastructure) the worker node.\n\n\n\nChanges since version 1.24.9_1548\n\n Component Previous Current Description \n\n Ubuntu 18.04 packages N/A N/A N/A \n Ubuntu 20.04 packages N/A N/A N/A \n Kubernetes N/A N/A N/A \n\n\n\n\n\n\n\n Change log for worker node fix pack 1.24.9_1548, released 19 December 2022 \n\nThe following table shows the changes that are in the worker node fix pack 1.24.9_1548. Worker node patch updates can be applied by updating, reloading (in classic infrastructure), or replacing (in VPC infrastructure) the worker node.\n\n\n\nChanges since version 1.24.8_1546\n\n Component Previous Current Description \n\n Containerd 1.6.10 1.6.12 See the [1.6.12 change log](https://github.com/containerd/containerd/releases/tag/v1.6.12), the [1.6.11 change log](https://github.com/containerd/containerd/releases/tag/v1.6.11), and the security bulletin for [CVE-2022-23471](https://www.ibm.com/support/pages/node/6850799).",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-changelog_124",
          "score": 25.055113,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724168651
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "How can i update a classic worker node?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "I'm sorry for any confusion, but the document provided does not contain information on how to update a classic worker node. It mainly focuses on gathering worker node data and troubleshooting steps for worker nodes in critical or not ready state."
    },
    {
      "speaker": "user",
      "text": "Major. menor update.",
      "timestamp": 1724166019,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Summarization"
        ],
        "Multi-Turn": [
          "Clarification"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": " To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x).  A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             ",
      "timestamp": 1724166034,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.470383,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724166608
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 50.23218,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724676697
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.73401,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724168625
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10642-7855-9754",
          "text": "\n* Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.\n\n\n\nFor more information, see [Update types](https://cloud.ibm.com/docs/containers?topic=containers-cs_versionsupdate_types).\n\nWhat happens to my apps during an update?\n: If you run apps as part of a deployment on worker nodes that you update, the apps are rescheduled onto other worker nodes in the cluster. These worker nodes might be in a different worker pool, or if you have stand-alone worker nodes, apps might be scheduled onto stand-alone worker nodes. To avoid downtime for your app, you must ensure that you have enough capacity in the cluster to carry the workload.\n\nHow can I control how many worker nodes go down at a time during an update or reload?\n: If you need all your worker nodes to be up and running, consider [resizing your worker pool](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-clics_worker_pool_resize) or [adding stand-alone worker nodes](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-clics_worker_add) to add more worker nodes. You can remove the additional worker nodes after the update is completed.\n\nIn addition, you can create a Kubernetes config map that specifies the maximum number of worker nodes that can be unavailable at a time, such as during an update. Worker nodes are identified by the worker node labels. You can use IBM-provided labels or custom labels that you added to the worker node.\n\nThe Kubernetes config map rules are used for updating worker nodes only.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-update",
          "score": 15.150766,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724166681
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What means a Major. minor update?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10642-6354-8294",
          "text": "\nYou notice that an update is available for your worker nodes in a [classic infrastructure](https://cloud.ibm.com/docs/openshift?topic=openshift-infrastructure_providers) cluster. What does that mean? As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version.\n\nApplies to only classic clusters. Have a VPC cluster? See [Updating VPC worker nodes](https://cloud.ibm.com/docs/openshift?topic=openshift-updatevpc_worker_node) instead.\n\nFor the latest security patches and fixes, make sure to update your worker nodes to the latest patch as soon as possible after it is available. For more information about the latest updates, review the [change log](https://cloud.ibm.com/docs/containers?topic=containers-changelog).\n\nTo update Satellite worker nodes, see [Updating hosts that are assigned as worker nodes to Satellite-enabled services](https://cloud.ibm.com/docs/satellite?topic=satellite-host-update-workers).\n\n\n\n* Patch: A worker node patch update includes security fixes. You can update the classic worker node to the latest patch by using the ibmcloud oc worker reload or update commands. Keep in mind that the update command also updates the worker node to the same major.minor version as the master and latest patch version, if a major.minor version update is also available.\n* Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-update",
          "score": 12.376706,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724166709
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What means a Major. minor update?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_11770-14487-16386",
          "text": "\n[Assign the newly attached hosts to your Satellite resource](https://cloud.ibm.com/docs/satellite?topic=satellite-assigning-hostshost-assign-manual). These hosts automatically receive the update when you assign them.\n4. After the new hosts are successfully assigned to your Satellite resource, [remove and delete the old hosts that you previously noted](https://cloud.ibm.com/docs/satellite?topic=satellite-host-remove).\n\n\n\n\n\n\n\n Updating worker node hosts in the Red Hat OpenShift on IBM Cloud console \n\nYou can update worker node hosts by using the Red Hat OpenShift on IBM Cloud console.\n\n\n\n1. Log in to the IBM Cloud console and click OpenShift > Clusters.\n2. Click the cluster where the hosts that you want to update are assigned and navigate to the Worker nodes page.\n3. Select each host that you want to update. After you select the hosts, an Update option appears.\n4. Click Update. In the dialog box that appears, click Update again. A message appears that the update started successfully.\n5. Wait while the hosts update. The update process for each host is complete when the Status of the host returns to Normal and the new version is listed in the Version column.\n\n\n\n\n\n\n\n Determining if the worker node version update is a major, minor, or patch update \n\nThe process to update a worker node is the same for all types of updates. However, you can find information on whether the update is a major, minor, or patch update.\n\nTo determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the [Red Hat OpenShift version change log](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift_versions). Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).",
          "url": "https://cloud.ibm.com/docs/satellite?topic=satellite-host-update-workers",
          "score": 12.378856,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724166736
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What means a Major. minor update?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "To perform a major.minor update on a worker node in a classic infrastructure cluster, you need to follow these steps:\n1. Log in to your account and set the context for your cluster.\n2. Back up your app data before starting the update process.\n3. Update the cluster master first if you are updating your worker nodes to a new major version. Use the command `ibmcloud oc cluster master update --cluster CLUSTER --version MAJOR.MINOR.PATCH --force-update`.\n4. Wait until the master update finishes.\n5. Cordon, drain, and replace each worker node sequentially.\n6. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, edit the `ocscluster` resource to include the new worker node after replacing it.\nNote that skipping versions during an upgrade is not supported.\nAlso, before updating your worker nodes, make sure to check the conditions of your worker nodes and cluster, including the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload."
    },
    {
      "speaker": "user",
      "text": "parts of a tag.",
      "timestamp": 1724167148,
      "enrichments": {
        "Question Type": [
          "Factoid",
          "Keyword"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy.",
      "timestamp": 1724167155,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724167726
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724167729
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724167738
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag."
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_09410-3448-5457",
          "text": "\nA tag consists of multiple parts:\n\nX.Y.Z-<date>.[hash]\n\nWhere\n\n\n\n* X represents the major version of an image.\n* Y represents the minor version of an image.\n* Z represents an incremental ID that determines the latest patched minor version.\n* <date> represents the date that the image is built and available. The format is YYYYMMDD.\n* [hash] represents the digest (manifest) of the container image. It is a unique SHA-256 hash.\n\n\n\nThe following table outlines the tagging convention adopted and the agent update behavior:\n\n\n\nTable 1. logging agent tags explained\n\n Tag Logging agent auto-update enabled More info \n\n X YES The logging agent auto-updates when a new minor version releases. <br>The logging agent does not update to a new major version, as these updates may require configuration changes. \n X.Y YES The logging agent auto-updates when a new patch version is released. \n X.Y.Z YES The logging agent auto-updates when a new vulnerability fix is released. The agent code does not change, but the included libraries have vulnerability fixes. \n X.Y.Z-<date>.[hash] NO The logging agent never updates. If you use this tag, make sure you are watching for new agent releases that have vulnerability fixes. \n\n\n\nDepending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. For example:\n\n\n\n* In a development environment, you can use a tag X and let auto-updates happen as new minor versions are released.\n* In a staging environment, you might consider using a tag X.Y so auto-updates happen when a new patch is released.\n* In a production environment, you can use the tag X.Y.Z so that auto-updates happen when a new vulnerability fix is released.\n* For highly regulated environments, you should use the tag X.Y.Z-<date>.[hash]. Notice that you will have to check periodically for vulnerability fixes, patches, and minor version releases to keep the agent free of issues.",
          "url": "https://cloud.ibm.com/docs/log-analysis?topic=log-analysis-log_analysis_agent",
          "score": 15.22839,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724168336
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What are the multiple parts of a tag "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_09275-13854-15898",
          "text": "\nTags are case-sensitive, and the maximum length of a tag is 128 characters.\n\n\n\n* The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon.\n* Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing.\n* A comma separates tags and can't be used within the tag name itself.\n\n\n\nIf you add PII information in the name, you might be disclosing sensitive data to others in the same account.\n\nWhen you define your tags, do not add sensitive information in the tag name.\n\nTags are visible to all members of an account.\n\nTo control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.\n\n\n\n\n\n\n\n Define the log ingestion strategy \n\nFor non-IBM Cloud enabled services, you must decide the method to collect and forward logs from a log source that you want to monitor to a logging instance.\n\nIn IBM Log Analysis, you can collect and forward data to a logging instance by using any of the following methods:\n\n\n\n* logging agent: Logging agent that automatically collects and forwards logs to 1 logging instance in your account.\n* Syslog: Logging daemon that collects information across multiple devices and system-services, and forwards logs to 1 logging instance in your account.\n* REST API: API that you can use to send log data and custom metadata to 1 logging instance in your account.\n* Code libraries: Libraries that you can use to code ingestion of logs from your apps and services to 1 logging instance. logging offer libraries for Node.JS, Python, Rails, Ruby, Go, iOS, Java, and PHP.\n\n\n\nFor any method that you adopt, you have the flexibility to choose the logging instance where you want to send data per log source. Decide how many instances you might need to collect data from all your log sources based on who can see the data and the type of data that is collected. Avoid sending data to a logging instance that has the platform logs flag enabled.",
          "url": "https://cloud.ibm.com/docs/log-analysis?topic=log-analysis-adoption",
          "score": 12.817389,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724168517
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What are the multiple parts of a tag "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_02237-4-2121",
          "text": "\n* UI\n* CLI\n* API\n* Terraform\n\n\n\n\n\n\n\n Working with tags \n\nUse tags to organize, track usage costs, and even manage access to your resources and service IDs. You can tag related resources and view them throughout your account by filtering by tags from your resource list.\n\nTo see a full list of tags in your account, go to Manage > Account in the IBM Cloud\u00ae console, and select Tags.\n\nYou can apply user tags to organize your resources and service IDs and easily find them later. User tags can also help you with identifying specific team usage or cost allocation. By creating access management tags, you can control access to your resources and service IDs without requiring updates to your IAM policies.\n\n\n\n Tagging rules \n\nTags are not case-sensitive, and the maximum length of a tag is 128 characters. The permitted characters are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. The only supported format for access management tags is key:value. The use of a colon formats the tag into a string that isolates two logical parts, like a env:dev pair. A comma separates multiple tags and can't be used within the tag name itself.\n\nTags are visible account-wide and can be replicated across geographic regions. Since tags are not regulated information, avoid creating tags that use personal information, such as your name, address, phone number, email address, or other identifying or proprietary information.\n\n\n\n Sample tags and syntax \n\nYou can apply tags to help you organize and manage your resources, service IDs, and access policies. Consider writing tags as key:value pairs to help coordinate your development environments, projects, compliance, and optimization throughout your organization. See the following table for some examples of tags that you might want to use.\n\n\n\nTable 1. Tag syntax\n\n Tag Description \n\n env:dev, env:test, env:stage, env:prod Use to identify or even manage access to your development environment \n project:lw-wizard, app:poc-app Use to identify or even manage access to a project \n dataresidency:germany, compliance:hipaa, compliance:pii Use to define compliance requirements",
          "url": "https://cloud.ibm.com/docs/account?topic=account-tag&interface=ui",
          "score": 11.31559,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724168539
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What are the multiple parts of a tag "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_05074-7-1939",
          "text": "\nTagging objects in IBM Cloud Object Storage \n\nYour data can be expressly defined, categorized, and classified in IBM Cloud\u00ae Object Storage using associated metadata, called \"tags.\" This document will show you how to take full control in \"tagging\" the objects representing your data.\n\n\n\n Objects and metadata \n\nOrganizing your data can be a complex task. Basic methods, such as using key prefixes like organizational \"folders\" are a great start to hierarchical structures. But for more complex organization, you will need custom \"\n\ntags.\" Your metadata can describe the relationships inherent to your data, and provide more organization than titles or folders. Unlike mere labels, there are two parts to a tag: a key and a value, defined individually according to your needs.\n\n\n\n Tagging Objects \n\nManaging tags describing your objects can be performed through various interfaces and architectures. Using the [Console](https://cloud.ibm.com) provides a graphical user interface. Using the command line requires tools like [curl](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-curl) and the knowledge of how it interacts with Object Storage.\n\n\n\n\n\n Before you begin \n\nYou need:\n\n\n\n* An [IBM Cloud\u00ae Platform account](https://cloud.ibm.com/login)\n* An [instance of IBM Cloud Object Storage](https://cloud.ibm.com/docs/cloud-object-storage/basics?topic=cloud-object-storage-provision) and a bucket created for this purpose\n* An [IAM API key](https://cloud.ibm.com/docs/cloud-object-storage/iam?topic=cloud-object-storage-iam-overview) with Writer access to your Object Storage bucket or instance\n* Either existing or new objects that will have tags applied to them.\n\n\n\n\n\n\n\n Reading tags \n\nTags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.",
          "url": "https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-object-tagging",
          "score": 10.38219,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724168588
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "What are the multiple parts of a tag "
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "A tag in the context of container images typically consists of several parts separated by colons or dots. These parts usually represent different aspects of the image, such as the name of the software, the version number, and any additional identifiers or qualifiers. For example, in the tag \"mysoftware:1.2.3-beta\", \"mysoftware\" is the name of the software, \"1.2.3\" is the version number, and \"-beta\" is an identifier indicating that this is a beta release. However, the exact format and meaning of the parts can vary depending on the naming conventions used by the software vendor or community."
    },
    {
      "speaker": "user",
      "text": "node data",
      "timestamp": 1724670283,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes.    For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   ",
      "timestamp": 1724670288,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724671101
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724671103
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724671105
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_03884-12444-14594",
          "text": "\nThere are three types of nodes: CA, peer, and ordering node.\n\n\n\n\n\n Ordering node \n\nThe node that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. These blocks are then distributed to peers, which then verify the blocks and add them to the ledgers on each channel. Ordering nodes contain the cryptographic identity material that is tied to each member and authenticate the identity of clients and peers to access the network. The overall function that an ordering node or collection of nodes provides is known as the ordering service.\n\n\n\n\n\n Organization \n\nSee [Member](https://cloud.ibm.com/docs/blockchain?topic=blockchain-glossaryglossary-member).\n\n\n\n\n\n Out of band \n\nAn expression used to refer to sharing network artifacts outside of the console UI, for example by email or some other file transfer mechanism. After submitting a smart contract proposal, the originator can share the smart contract package and package id with other channel members in an out of band operation by emailing this information to them.\n\n\n\n\n\n Peer \n\nA blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.\n\n\n\n\n\n Propose \n\nReferring to a step in the Hyperledger Fabric v2.x smart contract lifecycle, a smart contract definition is proposed to a channel for approval by the organizations that are specified in the channel lifecycle endorsement policy. A smart contract proposal contains a smart contract definition name, version, package, endorsement policy, and optionally a private data collection.\n\n\n\n\n\n Quorum \n\nIn a Raft ordering service, a quorum represents the number of nodes that must be available for transactions to be processed. This number is a majority of the total number of nodes in the consenter set of the channel. In other words, if you have one node, you need that node available to have a quorum, because the majority of one is one.",
          "url": "https://cloud.ibm.com/docs/blockchain?topic=blockchain-glossary",
          "score": 15.781107,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724670921
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "type node data"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_03972-7381-9534",
          "text": "\nA CA is the node that issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions. For more information about your CA and the identities that you will need to create, see [Managing identities](https://cloud.ibm.com/docs/blockchain?topic=blockchain-ibp-console-identitiesibp-console-identities).\n\nIn this tutorial, we will create one organization. Therefore, we will need to create one CA.\n\n\n\n Creating your peer organization CA \n\nYour CA issues the certificates and private keys for your organization's admins, client applications, and nodes. These identities are not managed by IBM and the keys are not stored in the console. They are only stored in your browser local storage. Therefore, make sure to export your identities and the MSP of your organization. If you attempt to access the console from a different machine or a different browser, you will need to import these identities and organization definitions.\n\nPerform the following steps from your console:\n\n\n\n1. Navigate to the Nodes tab on the left and click Add Certificate Authority. The side panels will allow you to customize the CA that you want to create and the organization that this CA will issue keys for.\n2. In this tutorial, we're creating nodes, so make sure the option to Create a Certificate Authority is selected. Then click Next.\n3. Use the side panel to give your CA a display name. Our recommended value for this CA is Org2 CA. Then give your CA admin credentials by specifying a CA administrator enroll ID of admin and a secret of adminpw.\n4. The Advanced deployment options are only available in paid clusters and can be safely ignored for purposes of this tutorial. For more information about these options, see the links below.",
          "url": "https://cloud.ibm.com/docs/blockchain?topic=blockchain-ibp-console-join-network",
          "score": 16.518295,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724671090
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "CA node data"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node. Additionally, you should check the CPU and memory level of your nodes and ensure that they are not over 80% usage. If they are, you may need to provision more nodes or reduce your workload. When you have gathered this information, include it in your support ticket when you open one."
    },
    {
      "speaker": "user",
      "text": "NodeSync",
      "timestamp": 1724671191,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           ",
      "timestamp": 1724671194,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724671542
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724675950
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724671551
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_06320-2897-4555",
          "text": "\n[NodeSync](https://docs.datastax.com/en/dse/6.7/dse-admin/datastax_enterprise/config/aboutNodesync.html) is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. Traditional repairs are unnecessary, as the automation ensures that NodeSync handles the repairs for you.\n\nWhile the automation enables NodeSync, you can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary:\n\nCREATE TABLE myTable (...) WITH nodesync = { 'enabled': 'true'};\n\nFor more information, see [enabling the NodeSync service](https://docs.datastax.com/en/dse/6.7/dse-admin/datastax_enterprise/config/enablingNodesync.html).\n\nNodetool is unsupported. Manual repairs that are issued against any table that is enabled with the NodeSync service [are ignored](https://docs.datastax.com/en/opscenter/6.5/opsc/online_help/services/opscNodeSyncService.htmlNodeSyncServiceversusRepairService). See error:\n\nWARNING: A manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run if a NodeSync-enabled table is targeted.\n\n\n\n\n\n Recommendations \n\n\n\n Benchmark before production \n\n\n\n* Do not use cqlsh and COPY for benchmarking, as COPY does not mimic typical client behavior. Instead, [nosqlbench](https://github.com/nosqlbench/nosqlbench) can be used for benchmarking.\n\n\n\n\n\n\n\n Data migrations \n\n\n\n* DSBULK is recommended for data migration.",
          "url": "https://cloud.ibm.com/docs/databases-for-cassandra?topic=databases-for-cassandra-getting-started",
          "score": 19.821285,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724671711
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "NodeSync"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "NodeSync is a feature in IBM Cloud Kubernetes Service that helps keep worker nodes in sync with the control plane. It automates the process of updating and maintaining worker nodes, reducing the manual work required to keep the cluster up-to-date. NodeSync can handle both major and minor updates, and it is designed to minimize disruptions to your workloads during the update process. It is important to note that NodeSync should only be used for clusters that were created with the IBM Cloud Kubernetes Service, and not for clusters created with other Kubernetes platforms."
    },
    {
      "speaker": "user",
      "text": "Worker node",
      "timestamp": 1724672458,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": " If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. ",
      "timestamp": 1724672477,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724675937
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724674377
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724674371
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10510-17837-19983",
          "text": "\nWorker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software.\n\n\n\n Who owns the worker node and am I responsible to secure it? \n\nThe ownership of a worker node depends on the type of cluster that you create and the infrastructure provider that you choose.\n\n\n\n* Classic clusters: Worker nodes are provisioned in to your IBM Cloud account. The worker nodes are dedicated to you and you are responsible to request timely updates to the worker nodes to ensure that the worker node OS and IBM Cloud Kubernetes Service components apply the latest security updates and patches.\n* VPC clusters: Worker nodes are provisioned in to an IBM Cloud account that is owned by IBM to enable monitoring of malicious activities and apply security updates. You can't access your worker nodes by using the VPC dashboard. However, you can manage your worker nodes by using the IBM Cloud Kubernetes Service console, CLI, or API. The virtual machines that make up your worker nodes are dedicated to you and you are responsible to request timely updates so that your worker node OS and IBM Cloud Kubernetes Service components apply the latest security updates and patches.\n\n\n\nFor more information, see [Your responsibilities by using Red Hat OpenShift on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-responsibilities_iks).\n\nUse the ibmcloud oc worker update[command](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-clics_worker_update) regularly (such as monthly) to deploy updates and security patches to the operating system and to update the Red Hat OpenShift version that your worker nodes run. When updates are available, you are notified when you view information about the master and worker nodes in the IBM Cloud console or CLI, such as with the ibmcloud oc clusters ls or ibmcloud oc workers ls --cluster <cluster_name> commands. Worker node updates are provided by IBM as a full worker node image that includes the latest security patches.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-security",
          "score": 21.317507,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724674167
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_06079-10297-12436",
          "text": "\nWorker node With IBM Cloud Kubernetes Service, the virtual machines that your cluster manages are instances that are called worker nodes. These worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers. However, the underlying hardware is shared with other IBM customers. For more information, see [Virtual machines](https://cloud.ibm.com/docs/containers?topic=containers-planning_worker_nodesvm). You manage the worker nodes through the automation tools that are provided by IBM Cloud Kubernetes Service, such as the API, CLI, or console. Unlike classic clusters, you don't see VPC compute worker nodes in your infrastructure portal or separate infrastructure bill, but instead manage all maintenance and billing activity for the worker nodes from IBM Cloud Kubernetes Service. Worker nodes include the same [components](https://cloud.ibm.com/docs/containers?topic=containers-service-archworker-components) as described in the Classic architecture. Community Kubernetes worker nodes run on Ubuntu 18.04 x86_64, 16.04 x86_64 (deprecated). \n Cluster networking Your worker nodes are created in a VPC subnet in the zone that you specify. By default, the public and private cloud service endpoints for your cluster are enabled. Communication between the master and worker nodes is over the private network. Authenticated external users can communicate with the master over the public network, such as to run kubectl commands. You can optionally set up your cluster to communicate with on-prem services by setting up a VPC VPN on the private network. \n App networking You can create a Kubernetes LoadBalancer service for your apps in the cluster, which automatically provisions a VPC load balancer in your VPC outside the cluster. The load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. For more information, see [Exposing apps with VPC load balancers](https://cloud.ibm.com/docs/containers?topic=containers-vpc-lbaas). Calico is used as the cluster networking policy fabric.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-service-arch",
          "score": 21.295753,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724673853
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10488-8451-10309",
          "text": "\nWorker nodes <br><br> * Provide worker node patch operating system (OS), version, and security updates.<br> * Fulfill automation requests to update and recover worker nodes.<br><br><br> <br><br> * Use the API, CLI, or console tools to [apply](https://cloud.ibm.com/docs/containers?topic=containers-updateupdate) the provided worker node updates that include operating system patches; or to request that worker nodes are rebooted, reloaded, or replaced.<br><br><br> \n Cluster version <br><br> * Provide a suite of tools to automate cluster management, such as the Red Hat OpenShift on IBM Cloud [API](https://containers.cloud.ibm.com/global/swagger-global-api//), [CLI plug-in](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-cli), and [console](https://cloud.ibm.com/kubernetes/clusters).<br> * Automatically apply Red Hat OpenShift master patch OS, version, and security updates.<br> * Make major and minor updates for master nodes available for you to apply.<br> * Provide worker node major, minor, and patch OS, version, and security updates.<br> * Fulfill automation requests to update cluster master and worker nodes.<br><br><br> <br><br> * Use the API, CLI, or console tools to [apply](https://cloud.ibm.com/docs/containers?topic=containers-updateupdate) the provided major and minor Red Hat OpenShift master updates and major, minor, and patch worker node updates.<br><br><br> \n\n\n\n\n\n\n\n Identity and access management \n\nYou and IBM share responsibilities for controlling access to your Red Hat OpenShift on IBM Cloud instances. For IBM Cloud\u00ae Identity and Access Management responsibilities, consult that product's documentation. You are responsible for identity and access management to your application data.\n\n\n\nTable 4. Responsibilities for identity and access management\n\n Resource IBM responsibilities Your responsibilities",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-responsibilities_iks",
          "score": 20.858765,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724674397
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Worker node"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_07578-398678-400724",
          "text": "\nFor more information, see [Updating the master](https://cloud.ibm.com/docs/containers?topic=containers-updatemaster).\n\nWorker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. The worker nodes are dedicated to your account and you are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Security updates and patches are made available by IBM Site Reliability Engineers (SREs) who continuously monitor the Linux image that is installed on your worker nodes to detect vulnerabilities and security compliance issues. For more information, see [Updating worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-updateworker_node).\n* Are the master and worker nodes highly available?\n\nThe Red Hat OpenShift on IBM Cloud architecture and infrastructure is designed to ensure reliability, low processing latency, and a maximum uptime of the service. By default, every cluster in Red Hat OpenShift on IBM Cloud is set up with multiple Red Hat OpenShift master instances to ensure availability and accessibility of your cluster resources, even if one or more instances of your Red Hat OpenShift master are unavailable.\n\nYou can make your cluster even more highly available and protect your app from a downtime by spreading your workloads across multiple worker nodes in multiple zones of a region. This setup is called a [multizone cluster](https://cloud.ibm.com/docs/openshift?topic=openshift-ha_clustersmz-clusters) and ensures that your app is accessible, even if a worker node or an entire zone is not available.\n\nTo protect against an entire region failure, create [multiple clusters and spread them across IBM Cloud regions](https://cloud.ibm.com/docs/openshift?topic=openshift-ha_clustersmultiple-clusters-glb). By setting up a network load balancer (NLB) for your clusters, you can achieve cross-region load balancing and cross-region networking for your clusters.",
          "url": "https://cloud.ibm.com/docs/faqs",
          "score": 20.194788,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724674341
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Worker node"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "To gather the relevant worker node data, follow these steps:\n1. Check the conditions of your worker nodes and cluster before you gather data. Specifically, check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n2. If all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n3. Before opening a support ticket, review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state.\n4. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node.\nRegarding the update of a classic worker node, you can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation, you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\nA major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command."
    },
    {
      "speaker": "user",
      "text": "Red Hat OpenShift",
      "timestamp": 1724674419,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "Red Hat OpenShift is a  Kubernetes container platform that provides a trusted environment to run enterprise workloads. It extends the Kubernetes platform with built-in software to enhance app lifecycle development, operations, and security. There is the Red Hat OpeShfit on IBM Cloud service, where  IBM sets up and helps you manage a cluster of worker nodes that come installed with the OpenShift Container Platform container orchestration software. Your Red Hat OpenShift workloads can scale across IBM\u2019s global footprint of data centers and multizone regions. At the same time, you\u2019re able to uniformly monitor, log, and secure apps. Additionally  is designed especially to accelerate your containerized app delivery processes that run on a Red Hat Enterprise Linux 7 operating system. You can orchestrate and scale your existing Red Hat OpenShift workloads across on-prem and off-prem clouds for a portable, hybrid solution that works the same in multicloud scenarios. ",
      "timestamp": 1724674424,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724676891
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724675127
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724675130
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10495-9135-10569",
          "text": "\n(https://cognitiveclass.ai/courses/docker-essentials)\n\n\n\n\n\n What is Red Hat OpenShift? \n\nRed Hat OpenShift is a Kubernetes container platform that provides a trusted environment to run enterprise workloads. It extends the Kubernetes platform with built-in software to enhance app lifecycle development, operations, and security. With Red Hat OpenShift, you can consistently deploy your workloads across hybrid cloud providers and environments. For more information about the differences between the community Kubernetes and Red Hat OpenShift cluster offerings, see the [comparison table](https://cloud.ibm.com/docs/openshift?topic=openshift-cs_ovopenshift_kubernetes).\n\n\n\n\n\n What compute host infrastructure does the service offer? \n\nYou can create clusters on Classic or IBM Cloud\u00ae Virtual Private Cloud infrastructure. You can also bring your own hosts by using Satellite.\n\nFor more information, see [Supported infrastructure providers](https://cloud.ibm.com/docs/openshift?topic=openshift-infrastructure_providers).\n\n\n\n\n\n Related resources \n\nReview how you can learn about Kubernetes concepts and the terminology.\n\n\n\n* Familiarize yourself with the product by completing the [Creating clusters tutorial](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift_tutorial).\n* Learn how Kubernetes and Red Hat OpenShift on IBM Cloud work together by completing this [course](https://cognitiveclass.ai/courses/kubernetes-course).",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-roks-overview",
          "score": 24.414478,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724675141
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10154-7-1896",
          "text": "\nBenefits and service offerings \n\n[Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u00ae](https://www.ibm.com/products/openshift) is an IBM Cloud service, where IBM sets up and helps you manage a cluster of worker nodes that come installed with the OpenShift Container Platform container orchestration software.\n\nTo see a list of the cluster operators that are supported by default, you can run the oc get co command.\n\n\n\n Benefits of using the service \n\nWith Red Hat OpenShift on IBM Cloud, your developers have a fast and secure way to containerize and deploy enterprise workloads in Kubernetes clusters. Red Hat OpenShift clusters build on Kubernetes container orchestration that offers consistency and flexibility for your development lifecycle operations.\n\nYour Red Hat OpenShift workloads can scale across IBM\u2019s global footprint of data centers and multizone regions. At the same time, you\u2019re able to uniformly monitor, log, and secure apps. Because IBM manages the service, you can focus on innovating with high-value IBM Cloud services and middleware, such as AI and analytics. You also have access to Red Hat packaged open-source tools, including your favorite app runtimes, frameworks, databases, and more.\n\nReady to get started? Try out the [creating a Red Hat OpenShift on IBM Cloud cluster tutorial](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift_tutorial).\n\nChoice of container platform provider\n: Deploy clusters with Red Hat OpenShift or community Kubernetes installed as the container platform orchestrator.\n: Choose the developer experience that fits your company, or run workloads across both Red Hat OpenShift or community Kubernetes clusters.\n: Built-in integrations from the IBM Cloud console to the Kubernetes dashboard or Red Hat OpenShift web console.\n: Single view and management experience of all your Red Hat OpenShift or community Kubernetes clusters from IBM Cloud.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-cs_ov",
          "score": 24.309294,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724675146
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_07578-396975-399135",
          "text": "\nWith Kubernetes, you can combine various resources such as secrets, deployments, and services to securely create and manage highly available, containerized apps.\n\nRed Hat OpenShift\n: Red Hat OpenShift on IBM Cloud is a Kubernetes-based platform that is designed especially to accelerate your containerized app delivery processes that run on a Red Hat Enterprise Linux 7 operating system. You can orchestrate and scale your existing Red Hat OpenShift workloads across on-prem and off-prem clouds for a portable, hybrid solution that works the same in multicloud scenarios. To get started, try out the [Red Hat OpenShift on IBM Cloud tutorial](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift_tutorial).\n* Does the service come with a managed Red Hat OpenShift master and worker nodes?\n\nEvery cluster in Red Hat OpenShift on IBM Cloud is controlled by a dedicated Red Hat OpenShift master that is managed by IBM in an IBM-owned IBM Cloud infrastructure account. The Red Hat OpenShift master, including all the master components, compute, networking, and storage resources, is continuously monitored by IBM Site Reliability Engineers (SREs). The SREs apply the latest security standards, detect and remediate malicious activities, and work to ensure reliability and availability of Red Hat OpenShift on IBM Cloud.\n\nPeriodically, Red Hat OpenShift releases [major, minor, or patch updates](https://cloud.ibm.com/docs/containers?topic=containers-cs_versionsupdate_types). These updates can affect the Red Hat OpenShift API server version or other components in your Red Hat OpenShift master. IBM automatically updates the patch version, but you must update the master major and minor versions. For more information, see [Updating the master](https://cloud.ibm.com/docs/containers?topic=containers-updatemaster).\n\nWorker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. The worker nodes are dedicated to your account and you are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches.",
          "url": "https://cloud.ibm.com/docs/faqs",
          "score": 23.226305,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724675890
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Red Hat OpenShift"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Red Hat OpenShift is a containerization platform that includes a comprehensive set of tools for developing, deploying, and managing containerized applications. It is built on top of Red Hat Enterprise Linux and Kubernetes and provides a consistent experience across hybrid and multi-cloud environments. OpenShift includes features such as built-in image registry, automated build and deployment, and centralized management of containerized applications. It also provides a developer-centric experience with features such as integrated development environment, source-to-image builds, and Jenkins pipelines. Additionally, OpenShift provides security features such as image scanning, network policies, and secret management."
    },
    {
      "speaker": "user",
      "text": "Changing a little bit of subject, what  would be the services of the Virtual Network Functions?",
      "timestamp": 1724677026,
      "enrichments": {
        "Question Type": [
          "Explanation",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "Network Function Virtualization (NFV) is the network infrastructure behind virtualizing network services such as routers, firewalls, and load balancers that were traditionally run on proprietary hardware. Those services, called Virtual Network Functions (VNFs), are packaged as virtual machines (VMs) on commodity hardware, which allows service providers to run their networks on standard servers instead of proprietary ones. The Virtual Network Function (VNF) data network interface is also attached to a VPC security group.  It ensure that the security group has inbound rules that allow traffic on the pool health port that is set up between the NLB and the VNF. Additionally the Virtual Network Function (VNF) must share one subnet with the Network Load Balancer (NLB).   Routing public internet \"ingress\" traffic and auto-scaling  services are not supported by VNF.\n                                 ",
      "timestamp": 1724677039,
      "contexts": [
        {
          "document_id": "ibmcld_06160-11142-12906",
          "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
          "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
          "score": 50.234882,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724677984
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift\n|assistant|: Red Hat OpenShift is a  Kubernetes container platform that provides a trusted environment to run enterprise workloads. It extends the Kubernetes platform with built-in software to enhance app lifecycle development, operations, and security. There is the Red Hat OpeShfit on IBM Cloud service, where  IBM sets up and helps you manage a cluster of worker nodes that come installed with the OpenShift Container Platform container orchestration software. Your Red Hat OpenShift workloads can scale across IBM\u2019s global footprint of data centers and multizone regions. At the same time, you\u2019re able to uniformly monitor, log, and secure apps. Additionally  is designed especially to accelerate your containerized app delivery processes that run on a Red Hat Enterprise Linux 7 operating system. You can orchestrate and scale your existing Red Hat OpenShift workloads across on-prem and off-prem clouds for a portable, hybrid solution that works the same in multicloud scenarios. \n|user|: Changing a little bit of subject, what  would be the services of the Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10596-11475-13230",
          "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
          "score": 49.99304,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724677986
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift\n|assistant|: Red Hat OpenShift is a  Kubernetes container platform that provides a trusted environment to run enterprise workloads. It extends the Kubernetes platform with built-in software to enhance app lifecycle development, operations, and security. There is the Red Hat OpeShfit on IBM Cloud service, where  IBM sets up and helps you manage a cluster of worker nodes that come installed with the OpenShift Container Platform container orchestration software. Your Red Hat OpenShift workloads can scale across IBM\u2019s global footprint of data centers and multizone regions. At the same time, you\u2019re able to uniformly monitor, log, and secure apps. Additionally  is designed especially to accelerate your containerized app delivery processes that run on a Red Hat Enterprise Linux 7 operating system. You can orchestrate and scale your existing Red Hat OpenShift workloads across on-prem and off-prem clouds for a portable, hybrid solution that works the same in multicloud scenarios. \n|user|: Changing a little bit of subject, what  would be the services of the Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10394-7-1848",
          "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
          "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
          "score": 49.552605,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "no",
                "timestamp": 1724677988
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node\n|assistant|:  If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. \n|user|: Red Hat OpenShift\n|assistant|: Red Hat OpenShift is a  Kubernetes container platform that provides a trusted environment to run enterprise workloads. It extends the Kubernetes platform with built-in software to enhance app lifecycle development, operations, and security. There is the Red Hat OpeShfit on IBM Cloud service, where  IBM sets up and helps you manage a cluster of worker nodes that come installed with the OpenShift Container Platform container orchestration software. Your Red Hat OpenShift workloads can scale across IBM\u2019s global footprint of data centers and multizone regions. At the same time, you\u2019re able to uniformly monitor, log, and secure apps. Additionally  is designed especially to accelerate your containerized app delivery processes that run on a Red Hat Enterprise Linux 7 operating system. You can orchestrate and scale your existing Red Hat OpenShift workloads across on-prem and off-prem clouds for a portable, hybrid solution that works the same in multicloud scenarios. \n|user|: Changing a little bit of subject, what  would be the services of the Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_14913-0-1238",
          "text": "\n\n\n\n\n\n\n  About virtual network functions over VPC \n\nNetwork Function Virtualization (NFV) is the network infrastructure behind virtualizing network services (such as routers, firewalls, and load balancers) that were traditionally run on proprietary hardware. These services, called Virtual Network Functions (VNFs), are packaged as virtual machines (VMs) on commodity hardware, which allows service providers to run their networks on standard servers instead of proprietary ones. This third-party software interacts with the IBM Software-Defined Networking (SDN) controller to offer easy configuration, centralized management, and lower operational costs.\n\nWorking along with IBM Cloud\u00ae Schematics (Infrastructure as Code) and the IBM Content catalog, customers are able to instantiate best-in-network solutions to manage their workload.\n\nBenefits include:\n\n\n\n*  Instantiating virtual network services and modifying configurations without the need to deploy new network hardware.\n*  Rapid service delivery with the agility to scale well above physical hardware.\n*  Centralized policy control.\n*  Using the same routers, firewalls, load balancers, and VPNs in IBM Cloud that were used in physical hardware or other cloud providers.\n\n\n\n\n\n\n\n\n",
          "url": "https://cloud.ibm.com/docs/vpc?topic=vpc-about-vnf",
          "score": 25.40456,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724678129
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "services of the Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_15151-0-569",
          "text": "\n\n\n\n\n\n\n  Configuring security groups \n\nThe Virtual Network Function (VNF) data network interface is attached to a VPC security group. Ensure that the security group has inbound rules that allow traffic on the pool health port that is set up between the NLB and the VNF. For example, if the health check is set up for TCP on port 80 (HTTP), then create an inbound rule for that security group. Additionally, you can create rules to allow or restrict data traffic.\n\n\n\n  Next step \n\n[Deploying a supported VNF](https://cloud.ibm.com/docs/vpc?topic=vpc-deploy-vnf)\n\n\n\n\n\n\n\n\n\n",
          "url": "https://cloud.ibm.com/docs/vpc?topic=vpc-configure-security-groups",
          "score": 19.275885,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724678179
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "services of the Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16026-0-358",
          "text": "\n\n\n\n\n\n\n  VNF limitations \n\nHigh Availability (HA) Virtual Network Function (VNF) deployments have the following known limitations.\n\n\n\n*  The Virtual Network Function (VNF) must share one subnet with the Network Load Balancer (NLB).\n*  Routing public internet \"ingress\" traffic to a VNF is not supported.\n*  Auto-scaling with the VNF is not supported.\n\n\n\n\n\n\n\n\n",
          "url": "https://cloud.ibm.com/docs/vpc?topic=vpc-vnf-limitations",
          "score": 23.046198,
          "feedback": {
            "relevant": {
              "868c6e58-57c1-48ee-8c67-832dbedcac18": {
                "value": "yes",
                "timestamp": 1724678731
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "Virtual Network Functions?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Virtual Network Functions (VNFs) are virtualized tasks that perform specific network functions, such as firewall protection, intrusion detection, and load balancing. Some common services provided by VNFs include:\n1. Firewall: This service filters incoming and outgoing network traffic based on predetermined security rules and policies.\n2. Intrusion Detection System (IDS): This service monitors network traffic for suspicious activities or policy violations and sends alerts to the network administrator.\n3. Load Balancer: This service distributes incoming network traffic across multiple servers or resources to improve network performance, reliability, and scalability.\n4. Virtual Private Network (VPN): This service creates a secure and encrypted connection between two or more networks over the internet.\n5. Deep Packet Inspection (DPI): This service examines the data packets passing through a network to detect threats, malware, or unauthorized access attempts.\n6. Content Filtering: This service blocks access to specific websites, applications, or content based on predefined policies.\n7. Network Address Translation (NAT): This service maps multiple private IP addresses to a single public IP address to conserve IP address space and improve network security.\n8. Quality of Service (QoS): This service prioritizes network traffic based on predefined policies to ensure optimal network performance for critical applications.\n9. Session Border Controller (SBC): This service manages and controls voice and video communications sessions between different networks or service providers.\n10. Wide Area Network (WAN) Optimization: This service improves network performance and reduces latency for applications and services accessed over wide area networks."
    }
  ],
  "status": "accepted",
  "status_history": [
    {
      "author": "868c6e58-57c1-48ee-8c67-832dbedcac18",
      "status": "created",
      "timestamp": 1724678955
    },
    {
      "author": "dc79eb51-e320-43c1-841c-3b1baa66d0aa",
      "status": "accepted",
      "timestamp": 1724945793
    },
    {
      "author": "868c6e58-57c1-48ee-8c67-832dbedcac18",
      "status": "accepted",
      "timestamp": 1725552271
    },
    {
      "author": "f6ce971c-68d2-4c26-b1c0-477dd28a5181",
      "status": "edited",
      "timestamp": 1726754969
    },
    {
      "author": "f6ce971c-68d2-4c26-b1c0-477dd28a5181",
      "status": "accepted",
      "timestamp": 1726755127
    }
  ],
  "editor": "f6ce971c-68d2-4c26-b1c0-477dd28a5181",
  "domain": "mt-rag-ibmcloud-elser-512-100-20240502",
  "reviewer": "f6ce971c-68d2-4c26-b1c0-477dd28a5181"
}