{
  "author": "f6ce971c-68d2-4c26-b1c0-477dd28a5181",
  "retriever": {
    "collection": {
      "name": "mt-rag-ibmcloud-elser-512-100-20240502",
      "size": "72442"
    },
    "parameters": {
      "max_count": 3,
      "max_utterances": -1,
      "query_syntax": "{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"text_expansion\": {\n          \"ml.tokens\": {\n            \"model_id\": \".elser_model_1\",\n            \"model_text\": \"${QUERY}\"\n          }\n        }\n      }\n    }\n  }\n}",
      "project": "{\n  \"text\": \"text\",\n  \"title\": \"title\",\n  \"url\": \"url\"\n}"
    }
  },
  "generator": {
    "id": "mistralai/mixtral-8x7b-instruct-v01",
    "name": "mixtral-8x7b-instruct-v01",
    "prompt": {
      "template": "[INST]\n${CONTEXT}\n${SYSTEM_INST}\n${INPUT}\n[/INST]\nanswer:",
      "input": "${SPEAKER}: ${TEXT}\n",
      "context": "[DOCUMENT]\n${TEXT}\n[END]\n",
      "system_instruction": "You are an AI Assistant, tasked with providing responses that are well-grounded in the provided documents. Given one or more documents and a user query, generate a response to the query. If no answer can be found in the documents, say, \"I do not have specific information\"."
    },
    "parameters": {
      "min_new_tokens": 1,
      "max_new_tokens": 512,
      "repetition_penalty": 1.05,
      "stop_sequences": [
        "<|endoftext|>"
      ]
    }
  },
  "messages": [
    {
      "speaker": "user",
      "text": "What is the Score Agreement level if it is less than 0?",
      "timestamp": 1724759091,
      "enrichments": {
        "Question Type": [
          "Factoid"
        ],
        "Multi-Turn": [
          "N/A"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "If the Score Agreement level is less than 0, it indicates poor agreement among the annotators, taking into account that what constitutes an acceptable level of agreement depends on your domain data and type system. This score can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.",
      "timestamp": 1724759094,
      "contexts": [
        {
          "document_id": "ibmcld_16481-3559-5682",
          "text": "\nTo help you identify areas that require investigation, scores that fall below the value that you specified for the inter-annotator agreement threshold are highlighted in red. In early stages of your annotation project, you might find that relation scores are often worse than mention scores because a perfect relation annotation requires that the mentions that define the relationship be in agreement first.\n\nThe score in the All column is a Fleiss Kappa score. It represents how consistently the same annotation was applied by multiple human annotators across all overlapping documents in the task. The value, which can range up to 1 and even be negative, can help you identify weaknesses in the annotation guidelines or particular human annotators. The following guidelines (Landis and Koch, 1977) provide a starting point for assessing overall performance.\n\nTable 1. Inter-annotator guidelines\n\n\n\n Score Agreement level \n\n < 0 Poor \n .01 -.20 Slight \n .21 -.40 Fair \n .41 - .60 Moderate \n .61 - .80 Substantial \n .81 - 1.0 Perfect \n\n\n\nThe score in the other columns is an F1 measure. It represents the level of annotation consistency between a pair of human annotators. The value can range from 0 to 1, where perfect agreement is indicated by the score 1. What constitutes an acceptable level of agreement depends on your domain data and type system. But to provide an example, here are the F1 thresholds that project managers expect to be met or exceeded in projects that are based on the KLUE type system:\n\n\n\n* Mentions with entity types: 0.85\n* Relations: 0.8\n* Coreference chains: 0.9\n\n\n\nInterpreting the scores depends on the complexity of your type system, the amount and complexity of the content annotated, how experienced the human annotators are, and other factors. For example, if the annotation task is focused on labeling parts of speech, you might expect to see a high score because of how well-defined the parts of speech are. But a task that requires deeper analysis of the text, where human interpretation is required, might show lower scores until you take steps to clarify the causes of the ambiguity.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-build-groundtruth",
          "score": 17.88838,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724760591
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "yes",
                "timestamp": 1726506332
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16417-3559-5683",
          "text": "\nTo help you identify areas that require investigation, scores that fall below the value that you specified for the inter-annotator agreement threshold are highlighted in red. In early stages of your annotation project, you might find that relation scores are often worse than mention scores because a perfect relation annotation requires that the mentions that define the relationship be in agreement first.\n\nThe score in the All column is a Fleiss Kappa score. It represents how consistently the same annotation was applied by multiple human annotators across all overlapping documents in the task. The value, which can range up to 1 and even be negative, can help you identify weaknesses in the annotation guidelines or particular human annotators. The following guidelines (Landis and Koch, 1977) provide a starting point for assessing overall performance.\n\nTable 1: Inter-annotator Guidelines\n\n\n\n Score Agreement level \n\n <0 Poor \n .01 - .20 Slight \n .21 - .40 Fair \n .41 - .60 Moderate \n .61 - .80 Substantial \n .81 - 1.0 Perfect \n\n\n\nThe score in the other columns is an F1 measure. It represents the level of annotation consistency between a pair of human annotators. The value can range from 0 to 1, where perfect agreement is indicated by the score 1. What constitutes an acceptable level of agreement depends on your domain data and type system. But to provide an example, here are the F1 thresholds that project managers expect to be met or exceeded in projects that are based on the KLUE type system:\n\n\n\n* Mentions with entity types: 0.85\n* Relations: 0.8\n* Coreference chains: 0.9\n\n\n\nInterpreting the scores depends on the complexity of your type system, the amount and complexity of the content annotated, how experienced the human annotators are, and other factors. For example, if the annotation task is focused on labeling parts of speech, you might expect to see a high score because of how well-defined the parts of speech are. But a task that requires deeper analysis of the text, where human interpretation is required, might show lower scores until you take steps to clarify the causes of the ambiguity.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-build-groundtruth",
          "score": 17.809696,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724760588
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16464-13891-15856",
          "text": "\nIn general, aim for a score of .8 out of 1, where 1 means perfect agreement. Because you annotated only two entity types in this tutorial, most of the entity type scores are N/A (not applicable), which means no information is available to give a score.\n\nFigure 1. Reviewing inter-annotator scores with users named dave and phil\n\n![This screen capture shows the inter-annotator scores for a task.](https://cloud.ibm.com/docs-content/v1/content/148d3bd95f946aa1bb53ea1540475f522e6b61c9/watson-knowledge-studio-data/images/wks_tutiaa2.png)\n4. After you review the scores, you can decide whether you want to approve or reject document sets that are in the SUBMITTED status. Take one of these actions:\n\n\n\n* If the scores are acceptable for an annotation set, select the check box and click Accept. Documents that do not overlap with other document sets are promoted to ground truth. Documents that do overlap must first be reviewed through adjudication so that conflicts can be resolved. For this tutorial, accept both document sets.\n* If the scores are not acceptable for an annotation set, select the check box and click Reject. The document set needs to be revisited by the human annotator to improve the annotations.\n\n\n\n\n\n\n\n\n\n Results \n\nWhen you evaluated the inter-annotator agreement scores, you saw how different pairs of human annotators annotated the same document. If the inter-annotator agreement score was acceptable, you accepted the document set.\n\n\n\n\n\n\n\n Lesson 6: Adjudicating conflicts in annotated documents \n\nIn this lesson, you will learn how to adjudicate conflicts in documents that overlap between document sets in Knowledge Studio.\n\n\n\n About this task \n\nWhen you approve a document set, only the documents that do not overlap with other document sets are promoted to ground truth. If a document is part of the overlap between multiple document sets, you must adjudicate any annotation conflicts before the document can be promoted to ground truth.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-wks_tutml_intro",
          "score": 11.87866,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724760586
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726506361
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency."
    },
    {
      "speaker": "user",
      "text": "How many annotators are needed?",
      "timestamp": 1724759855,
      "enrichments": {
        "Question Type": [
          "Explanation"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. That allows you to view the scores by comparing a pair of human annotators (for example, compare all of John's mention annotations to all of Mary's mention annotations). You can also view the scores by comparing specific documents (for example, compare the relation annotations that John made in one document to the relation annotations that Mary made in the same document).",
      "timestamp": 1724759858,
      "contexts": [
        {
          "document_id": "ibmcld_16425-14122-16407",
          "text": "\nAnother indicator of annotation inconsistency is if you have enough annotations, but the corpus density is low. Density can be impacted when a mention that is significant in the domain literature occurs often, but is annotated as different types across the document set.\n\nLow precision is often an indication that you need to improve annotation consistency. To do so, review the annotation guidelines, better train the human annotators, and ensure that human annotators are working in concert and not in isolation from one another.\n\nCheck the inter-annotator agreement score. This score, which measures the degree of agreement between different annotators' output on the same document, is a valuable number. Not only does this score tell you the quality of the ground truth documents that will be used to train the machine learning model, but it also indicates the upper bound of machine learning model performance. A model that is trained on these documents is unlikely to outperform the best agreement that humans can reach. For example, if performance persists at 75 and does not go higher, take a look at the inter-annotator agreement results. If the inter-annotator agreement score is 80, take actions to better train the human annotators and ensure that conflicts are correctly resolved (according to the annotation guidelines) during adjudication. If humans can't agree on how something should be labeled, then it's unlikely that a machine learning model will apply the correct labels.\n* Enhance human annotator guidelines\n\nClear and comprehensive annotator guidelines are a crucial part of a harmonious and successful annotation development effort. Human annotators have a tough job to do. There can be nuances in assigning entity and relation types that are hard to anticipate until you start to work with the domain documents. The guidelines can provide a sanity check to human annotators as they evaluate documents. The guidelines should be a living and changing document, especially at the beginning of the annotation process. They provide a key feedback loop because a human annotator can capture things she learned while annotating a few documents, then as she or someone else annotates a few more documents, new tips and gotchas can be added to the guideline, and so on.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
          "score": 23.490267,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724761996
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726507432
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16410-8324-10312",
          "text": "\nInter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set.\n\nYou cannot change the annotation set name after the set is created.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.\n* Open a task to add annotation sets to it. Ensure that the annotation sets that you add include documents that overlap with documents in the original annotation sets.\n\n\n\nFrom the Settings tab of the main navigation, you can specify the following information:\n\n\n\n* Specify preferences for using colors and keyboard shortcuts in the ground truth editor.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documents",
          "score": 16.722137,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724761994
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "inter annotator agreement"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16417-1764-4158",
          "text": "\nAs the project manager resolves annotation conflicts, the approved annotations are promoted to ground truth.\n\n\n\nKeep in mind that human annotation always requires judgment calls. The annotation guidelines can do a lot to ensure that text is correctly and consistently annotated, but even the best guidelines are open to interpretation by humans. To obtain ground truth, you will want to spend time training and educating human annotators so that they can make the best judgments when analyzing domain content.\n\n\n\n Inter-annotator agreement \n\nAfter human annotators have annotated the documents, you must determine which documents to promote to ground truth. Start by reviewing the inter-annotator agreement scores. Documents with low scores are candidates to be rejected and returned to the human annotator for improvement.\n\nWhen calculating inter-annotator agreement scores, the system examines all overlapping documents in all annotation sets in the task, regardless of the status of the sets. Because you cannot accept or reject annotation sets until they are in the Submitted status, you might not want to evaluate inter-annotator agreement until all annotation sets are submitted, or you might want to limit your review to only the human annotators who submitted their completed annotation sets.\n\nThe inter-annotator agreement scores show how different human annotators annotated mentions, relations, and coreference chains. You can view the scores by comparing a pair of human annotators (for example, compare all of John's mention annotations to all of Mary's mention annotations). You can also view the scores by comparing specific documents (for example, compare the relation annotations that John made in one document to the relation annotations that Mary made in the same document).\n\nTo help you identify areas that require investigation, scores that fall below the value that you specified for the inter-annotator agreement threshold are highlighted in red. In early stages of your annotation project, you might find that relation scores are often worse than mention scores because a perfect relation annotation requires that the mentions that define the relationship be in agreement first.\n\nThe score in the All column is a Fleiss Kappa score. It represents how consistently the same annotation was applied by multiple human annotators across all overlapping documents in the task.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-build-groundtruth",
          "score": 17.094318,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724761991
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "inter annotator agreement"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "The number of annotators needed can vary depending on the specific requirements of the project. However, to calculate inter-annotator agreement scores, at least two human annotators are required. Additionally, an annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\nIt's also important to note that an annotation set that will be used to train a machine learning model must contain at least 10 annotated documents, and inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents."
    },
    {
      "speaker": "user",
      "text": "data volume",
      "timestamp": 1724763580,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Non-Question",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.",
      "timestamp": 1724763584,
      "contexts": [
        {
          "document_id": "ibmcld_16487-7-2224",
          "text": "\nAdding documents for annotation \n\nTo train a machine learning model, you must add documents that contain subject matter knowledge, such as journal articles or other industry-specific texts, to your workspace.\n\n\n\n About this task \n\nThis section describes how to add documents for annotation only. To define rules for the rule-based model, you add or upload documents from which you can draw patterns to define as rules. See [Adding documents for defining rules](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-wks_rule_anno_add) for more information.\n\n\n\n\n\n Documents \n\nTo train a machine learning model, you need to collect documents that are representative of your domain content and of high value to your application.\n\nTry to ensure that your training documents are truly representative of content that is of interest to your domain; that is, they contain many relevant mentions that can be annotated. To choose the best documents, follow these guidelines:\n\n\n\n* Strive to provide a set of documents that have a total size of about 300,000 words. Provide more words for a complex type system, and fewer for a simpler one.\n* Limit each document to a page or two of content (fewer than 2,000 words, and closer to 1,000 words per document is best). In the early stages of model development, keeping each document down to a few paragraphs is also a good practice. A human annotator can mark mentions and relations in a long document, but attempts to mark coreferences across multiple pages might prove unwieldy.\n* Ensure that the data in the documents is distributed across all possible entity types, subtypes, and roles, and the relationships between them. A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.\n* Again, documents should represent the breadth of the subject matter that the application will cover, but in the case of skewed frequency-of-occurrence of entity types and relation types, try to get at least 50 exemplars of each type, more for entity types that have mentions which tend to be phrases.\n* The set that you create for training must contain at least 10 annotated documents.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-documents-for-annotation",
          "score": 18.297113,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724763831
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "documents size  for annotaion"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16483-9110-10998",
          "text": "\n<br><br> * CSV file in UTF-8 format<br> * Text in UTF-8 format<br> * HTML<br> * PDF files (scanned and password-protected files are not supported)<br> * Microsoft Word DOC or DOCX files (password-protected files are not supported)<br> * ZIP file that contains documents downloaded from another workspace<br> * ZIP file that contains documents in UIMA CAS XMI format<br><br><br> ZIP archive file of documents <br><br> * 40,000 characters per document<br> * 10,000 documents per workspace<br> * 1,000 document sets (including annotation sets) per workspace<br> * 5 MB per file and 200 MB per upload (TXT, PDF, DOC, DOCX, and HTML files)<br><br><br> \n Pre-annotation Use a dictionary or IBM Watson\u00ae Natural Language Understanding pre-annotator to provide a starting point for human annotation. <br> <br>You cannot re-annotate a corpus from IBM Watson Explorer. Raw documents. <br> <br>Note: Do not pre-annotate documents that a human annotator has already annotated, or you will lose the work done by the human annotator. Partly-annotated documents None \n Document annotation Manage human annotation. Annotate entities, relations, and coreference chains to create ground truth Annotation task Ground truth <br><br> * 256 active anntation tasks per workspace<br><br><br> \n Training and refinement Train a supervised machine learning model to extract domain-specific information from unstructured text. Evaluate and improve a supervised machine learning model. You cannot create a semi-supervised or unsupervised machine learning model. You cannot do extensive feature engineering. Not applicable Machine learning model <br><br> * 1 machine learning model per workspace<br> * 10 model versions per workspace<br> * Maximum number of workspaces is determined by your deployment.<br> * The maximum number of training actions you can perform per month is determined by your deployment.<br><br><br>",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-create-project",
          "score": 18.251581,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724763828
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726508092
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "documents size  for annotaion"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16423-7-2234",
          "text": "\nAdding documents for annotation \n\nTo train a machine learning model, you must add documents that contain subject matter knowledge, such as journal articles or other industry-specific texts, to your workspace.\n\n\n\n About this task \n\nThis section describes how to add documents for annotation only. To define rules for the rule-based model, you add or upload documents from which you can draw patterns to define as rules. See [Adding documents for defining rules](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-wks_rule_anno_add) for more information.\n\n\n\n\n\n Documents \n\nTo train a machine learning model, you need to collect documents that are representative of your domain content and of high value to your application.\n\nTry to ensure that your training documents are truly representative of content that is of interest to your domain; that is, they contain many relevant mentions that can be annotated. To choose the best documents, follow these guidelines:\n\n\n\n* Strive to provide a set of documents that have a total size of about 300,000 words. Provide more words for a complex type system, and fewer for a simpler one.\n* Limit each document to a page or two of content (fewer than 2,000 words, and closer to 1,000 words per document is best). In the early stages of model development, keeping each document down to a few paragraphs is also a good practice. A human annotator can mark mentions and relations in a long document, but attempts to mark coreferences across multiple pages might prove unwieldy.\n* Ensure that the data in the documents is distributed across all possible entity types, subtypes, and roles, and the relationships between them. A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.\n* Again, documents should represent the breadth of the subject matter that the application will cover, but in the case of skewed frequency-of-occurrence of entity types and relation types, try to get at least 50 exemplars of each type, more for entity types that have mentions which tend to be phrases.\n* The set that you create for training must contain at least 10 annotated documents.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-documents-for-annotation",
          "score": 18.100645,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724763825
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "documents size  for annotaion"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Based on the provided document, there are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document. During the early stages of model development, it is advisable to keep each document down to a few paragraphs. Additionally, the data in the documents should be distributed across all possible entity types, subtypes, and roles, and their relationships. A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection. The set of documents for training must contain at least 10 annotated documents."
    },
    {
      "speaker": "user",
      "text": "So, 3 ppl at least?",
      "timestamp": 1724766763,
      "enrichments": {
        "Multi-Turn": [
          "Clarification"
        ],
        "Question Type": [
          "Factoid"
        ],
        "Answerability": [
          "UNANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "I am not sure what you are asking. The minimal number of annotators needed is two, as per my answer to your previous question above. There can of course be more than two annotators depending on the project, but the minimal requirement is two annotators.",
      "timestamp": 1724766768,
      "contexts": [
        {
          "document_id": "ibmcld_16410-8324-10312",
          "text": "\nInter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set.\n\nYou cannot change the annotation set name after the set is created.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.\n* Open a task to add annotation sets to it. Ensure that the annotation sets that you add include documents that overlap with documents in the original annotation sets.\n\n\n\nFrom the Settings tab of the main navigation, you can specify the following information:\n\n\n\n* Specify preferences for using colors and keyboard shortcuts in the ground truth editor.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documents",
          "score": 41.94719,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724767083
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726508452
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16468-8522-10510",
          "text": "\nIf no annotation sets are available, click Create Annotation Sets.\n\n\n\n1. For the base set, select the document set or annotation set that you want to divide into annotation sets.\n2. For the overlap value, specify the percentage of documents that you want to include in each annotation set. Inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set. You cannot change the annotation set name after the set is created.\n\nThe maximum size of the annotation set name is 256 characters.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-annotate-documents",
          "score": 38.855537,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724767086
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726508476
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16417-7044-9159",
          "text": "\nYou can see which documents are in an annotation set by opening the Assets> Documents page, clicking the Annotation Sets tab, and then clicking the names of the sets.\n\nYou might experience situations in which no overlapping documents are found. This might happen, for example, if you create annotation sets in two rounds and add them to the same task. Even though the annotation sets were created at about the same time, they don't have any documents in common. For another example, if you create annotation sets with overlapping documents, but add one annotation set per task instead of adding all of the annotation sets to a single task, no overlapping documents will be found and inter-annotator agreement cannot be calculated.\n\n\n\n\n\n Procedure \n\nTo assess annotation agreement between human annotators:\n\n\n\n1. Log in as a Knowledge Studio administrator or project manager, and select your workspace.\n2. Select the Machine Learning Model > Annotations page, then click the Annotation Tasks tab. Open the task that you want to evaluate.\n3. Click Calculate Inter-Annotator Agreement. The default view shows agreement scores for how consistently pairs of human annotators annotated mentions. The top row shows the overall consistency between each pair of annotators, and the table below shows how consistently a pair of annotators labeled specific mentions in the text.\n4. To explore how consistently pairs of human annotators annotated relations and coreferences, select Relation or Coreference from the first menu.\n5. To explore how consistently a pair of human annotators annotated entities, relations, or coreferences in specific overlapping documents, select Document in the second menu and then select the pair of annotators that you want to evaluate.\n6. After reviewing the scores, you can decide whether you want to approve or reject annotation sets that are in Submitted status. After an annotation set is submitted, a check box is displayed next to its name. Take one of these actions:\n\n\n\n* If the inter-annotator agreement scores are acceptable for an annotation set, select the check box and click Accept.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-build-groundtruth",
          "score": 36.50021,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724767089
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726508495
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Yes, at least three people are typically needed to calculate inter-annotator agreement scores. This is because two or more human annotators need to annotate the same documents for these scores to be calculated. In the example provided, six documents (20%) are annotated by all human annotators, and the remaining 24 documents are divided among the 3 human annotators. Therefore, at least three people are involved in the annotation process."
    },
    {
      "speaker": "user",
      "text": "pre-annotations make the task easier",
      "timestamp": 1724767665,
      "enrichments": {
        "Question Type": [
          "Non-Question",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "Yes, pre-annotating documents can make the job of human annotators easier later by identifying potential entities or concepts in the text beforehand. Although it is an optional step, it can help to speed up the annotation process and increase consistency in the annotations  because it covers the straightforward annotations, and gets the job of annotating the documents underway.",
      "timestamp": 1724767676,
      "contexts": [
        {
          "document_id": "ibmcld_16444-3168-4952",
          "text": "\nYou can run a pre-annotator on documents that were added to the ground truth as part of the current workspace. Annotations that were added to the documents, reviewed, accepted, and promoted to ground truth within the current workspace are not stripped out.\n\n\n\n\n\n Running multiple pre-annotators \n\nKnowledge Studio allows you to run multiple pre-annotators at once. First, you need to prepare the pre-annotation methods that you want to use. For more information, see the following sections.\n\n\n\n* [Dictionaries](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannot)\n* [Machine learning model](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannotsire)\n* [Rules-based model](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannotrule)\n\n\n\n\n\n Configuring the order of pre-annotators \n\nWhen multiple pre-annotators are used, the first annotation made to a span of text is saved for the results, even if other pre-annotators attempt to annotate the same span of text later in the order. This doesn't apply to human annotations, which are preserved regardless of pre-annotation order.\n\nFor example, consider the example text IBM Watson. If a dictionary that is first in the order labels IBM as an Organization entity type, a machine learning model that is second in the order can't annotate IBM Watson as a Software Brand entity type because that would override the earlier annotation made to IBM.\n\nYou can view the current order of pre-annotators in the Order column on the Machine Learning Model > Pre-annotation page. To change the order, complete the following steps.\n\n\n\n1. Click Order Settings.\n2.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotation",
          "score": 14.38329,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724768552
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726581019
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "preannotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16507-3099-4772",
          "text": "\nIt can incorporate some of the function of the dictionary pre-annotator if you enable it, by identifying class types for dictionary terms that it finds in the documents also.\n\n\n\nAlternatively, you can upload already-annotated documents, and use them to start training the machine learning model. You cannot run a pre-annotator on annotated documents that you upload or the existing annotations will be stripped from the documents and replaced with annotations produced by the pre-annotator only.\n\n\n\n\n\n Running multiple pre-annotators \n\nKnowledge Studio allows you to run multiple pre-annotators at once. First, you need to prepare the pre-annotation methods that you want to use. For more information, see the following sections:\n\n\n\n* [Natural Language Understanding](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannotnlu)\n* [Dictionaries](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannot)\n* [Machine learning model](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannotsire)\n* [Rules-based model](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannotrule)\n\n\n\n\n\n Configuring the order of pre-annotators \n\nWhen multiple pre-annotators are used, the first annotation made to a span of text is saved for the results, even if other pre-annotators attempt to annotate the same span of text later in the order. This doesn't apply to human annotations, which are preserved regardless of pre-annotation order.\n\nFor example, consider the example text IBM Watson.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotation",
          "score": 13.890721,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724768549
              },
              "7ee1aecf-492d-4847-a436-3db915c57d2e": {
                "value": "no",
                "timestamp": 1726581126
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "preannotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16563-3072-4738",
          "text": "\nPre-annotating documents is an optional step. However, it is a worthwhile step because it makes the job of human annotators easier later.\n\nFor more information about pre-annotation with dictionaries, see [Pre-annotating documents with a dictionary](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannot).\n\n\n\n\n\n Procedure \n\n\n\n1. Within your workspace, click Assets > Dictionaries.\n\nThe Test dictionary dictionary opens. The [Adding a dictionary](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-wks_tutintrowks_tutless4) lesson of the Getting started with Knowledge Studio tutorial shows you how to create this dictionary.\n2. From the Entity type list, select the ORGANIZATION entity type to map it to the Test dictionary dictionary.\n\nThe [Creating a type system](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-wks_tutintrowks_tutless3) lesson of the Getting started with Knowledge Studio tutorial shows how to create the type system that contains the ORGANIZATION entity type.\n3. On the Machine Learning Model > Pre-annotation page, click Run Pre-annotators.\n4. Select Dictionary, then click Next.\n5. Select the document set that you created in [Lesson 1](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-wks_tutml_introtut_lessml1).\n6. Click Run.\n\n\n\n\n\n\n\n Results \n\nThe documents in the selected sets are pre-annotated by using the dictionary that you created. If you like, you can use the dictionary to pre-annotate document sets or annotation sets that you add later.\n\n\n\n\n\n\n\n Lesson 3: Creating an annotation task",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-wks_tutml_intro",
          "score": 13.23701,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724768546
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "preannotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16507-7-2044",
          "text": "\nBootstrapping annotation \n\nSimplify the job of the human annotator by pre-annotating the documents in a workspace. A pre-annotator is a Knowledge Studio dictionary, rule-based model, or machine learning model that you can run to find and annotate mentions automatically.\n\nPre-annotation makes the job of human annotators easier because it covers the straightforward annotations, and gets the job of annotating the documents underway.\n\nThe method that you use to pre-annotate documents in no way restricts the ways that you can use the resulting model. For example, just because you use the Natural Language Understanding service to pre-annotate documents does not mean you must deploy the final machine learning model that you build to the Natural Language Understanding service.\n\n\n\n Pre-annotation methods \n\nThe following pre-annotators are available:\n\n\n\n* Natural Language Understanding\n\nA pre-annotator that you can use to find mentions of entities in your documents automatically. If your source documents have general knowledge subject matter, then this pre-annotator is a good choice for you. If you are working with highly specialized documents that focus on a specific field, such as patent law research, for example, the dictionary pre-annotator or rule-based model might be a better choice.\n* Dictionary\n\nUses a dictionary of terms that you provide and associate with an entity type to find mentions of that entity type in the documents. This choice is best for fields with unique or specialized terminology because this pre-annotator does not analyze the context in which the term is used in the way a machine learning pre-annotator does; it instead relies on the term being distinct enough to have a decipherable meaning regardless of the context in which it is used. For example, it is easier to recognize asbestos as a mineral entity type than to determine the entity type of squash, which can refer to a vegetable, a sport, or a verb meaning to crush something.\n\nDictionary pre-annotators do not recognize entity subtypes.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotation",
          "score": 14.658876,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770545
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16444-7-2064",
          "text": "\nBootstrapping annotation \n\nSimplify the job of the human annotator by pre-annotating the documents in a workspace. A pre-annotator is a Knowledge Studio dictionary, rule-based model, or machine learning model that you can run to find and annotate mentions automatically.\n\nPre-annotation makes the job of human annotators easier because it covers the straightforward annotations, and gets the job of annotating the documents underway.\n\nThe method that you use to pre-annotate documents in no way restricts the ways that you can use the resulting model. For example, just because you use the Natural Language Understanding service to pre-annotate documents does not mean you must deploy the final machine learning model that you build to the Natural Language Understanding service.\n\n\n\n Pre-annotation methods \n\nThe following pre-annotators are available:\n\n\n\n* Dictionary\n\nUses a dictionary of terms that you provide and associate with an entity type to find mentions of that entity type in the documents. This choice is best for fields with unique or specialized terminology because this pre-annotator does not analyze the context in which the term is used in the way a machine learning pre-annotator does; it instead relies on the term being distinct enough to have a decipherable meaning regardless of the context in which it is used. For example, it is easier to recognize asbestos as a mineral entity type than to determine the entity type of squash, which can refer to a vegetable, a sport, or a verb meaning to crush something.\n\nDictionary pre-annotators do not recognize entity subtypes. Human annotators can specify entity subtypes for each pre-annotated mention by working on an [annotation task](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documentswks_hatask) with the pre-annotated document.\n* Machine learning\n\nUses a machine learning model to automatically annotate documents. This option is only available to you if you have created a machine learning model with Knowledge Studio already.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotation",
          "score": 14.207621,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770548
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Yes, pre-annotating documents can make the job of human annotators easier later by identifying potential entities or concepts in the text beforehand. This can help to speed up the annotation process and increase consistency in the annotations. For more information about pre-annotation with dictionaries, see [Pre-annotating documents with a dictionary](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotationwks_preannot)."
    },
    {
      "speaker": "user",
      "text": "many kinds of pre-anno! Use one at a time or combination?  also, different tool btn conversational and technical docs?",
      "timestamp": 1724769499,
      "enrichments": {
        "Question Type": [
          "Composite",
          "Explanation",
          "Factoid"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "Yes, there are indeed several types of pre-annotators available.  These pre-annotators can be used individually or in combination. Regarding the use of different tools for conversational and technical documents, for instance,  if your source documents have general knowledge subject matter, then Natural Language Understanding pre-annotator is a good choice for you. If you are working with highly specialized documents that focus on a specific field, such as patent law research, the dictionary pre-annotator or rule-based model might be a better choice. ",
      "timestamp": 1724769504,
      "contexts": [
        {
          "document_id": "ibmcld_16507-7-2044",
          "text": "\nBootstrapping annotation \n\nSimplify the job of the human annotator by pre-annotating the documents in a workspace. A pre-annotator is a Knowledge Studio dictionary, rule-based model, or machine learning model that you can run to find and annotate mentions automatically.\n\nPre-annotation makes the job of human annotators easier because it covers the straightforward annotations, and gets the job of annotating the documents underway.\n\nThe method that you use to pre-annotate documents in no way restricts the ways that you can use the resulting model. For example, just because you use the Natural Language Understanding service to pre-annotate documents does not mean you must deploy the final machine learning model that you build to the Natural Language Understanding service.\n\n\n\n Pre-annotation methods \n\nThe following pre-annotators are available:\n\n\n\n* Natural Language Understanding\n\nA pre-annotator that you can use to find mentions of entities in your documents automatically. If your source documents have general knowledge subject matter, then this pre-annotator is a good choice for you. If you are working with highly specialized documents that focus on a specific field, such as patent law research, for example, the dictionary pre-annotator or rule-based model might be a better choice.\n* Dictionary\n\nUses a dictionary of terms that you provide and associate with an entity type to find mentions of that entity type in the documents. This choice is best for fields with unique or specialized terminology because this pre-annotator does not analyze the context in which the term is used in the way a machine learning pre-annotator does; it instead relies on the term being distinct enough to have a decipherable meaning regardless of the context in which it is used. For example, it is easier to recognize asbestos as a mineral entity type than to determine the entity type of squash, which can refer to a vegetable, a sport, or a verb meaning to crush something.\n\nDictionary pre-annotators do not recognize entity subtypes.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-preannotation",
          "score": 14.658876,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770257
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16444-7-2064",
          "text": "\nBootstrapping annotation \n\nSimplify the job of the human annotator by pre-annotating the documents in a workspace. A pre-annotator is a Knowledge Studio dictionary, rule-based model, or machine learning model that you can run to find and annotate mentions automatically.\n\nPre-annotation makes the job of human annotators easier because it covers the straightforward annotations, and gets the job of annotating the documents underway.\n\nThe method that you use to pre-annotate documents in no way restricts the ways that you can use the resulting model. For example, just because you use the Natural Language Understanding service to pre-annotate documents does not mean you must deploy the final machine learning model that you build to the Natural Language Understanding service.\n\n\n\n Pre-annotation methods \n\nThe following pre-annotators are available:\n\n\n\n* Dictionary\n\nUses a dictionary of terms that you provide and associate with an entity type to find mentions of that entity type in the documents. This choice is best for fields with unique or specialized terminology because this pre-annotator does not analyze the context in which the term is used in the way a machine learning pre-annotator does; it instead relies on the term being distinct enough to have a decipherable meaning regardless of the context in which it is used. For example, it is easier to recognize asbestos as a mineral entity type than to determine the entity type of squash, which can refer to a vegetable, a sport, or a verb meaning to crush something.\n\nDictionary pre-annotators do not recognize entity subtypes. Human annotators can specify entity subtypes for each pre-annotated mention by working on an [annotation task](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documentswks_hatask) with the pre-annotated document.\n* Machine learning\n\nUses a machine learning model to automatically annotate documents. This option is only available to you if you have created a machine learning model with Knowledge Studio already.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotation",
          "score": 14.207621,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770261
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_10916-57815-59652",
          "text": "\npre-annotation \n\nThe process of annotating a set of documents prior to human annotation. Documents can be pre-annotated by using a rule-based model, a machine-learning model, IBM Watson Natural Language Understanding, or a dictionary. Pre-annotation can help human annotators more quickly prepare a set of ground truth documents.\n\n\n\n\n\n precision \n\nA measurement that specifies the proportion of results that are relevant. Precision, which is a positive predictive value, is determined by the number of correct positive results divided by the number of all positive results. Accuracy is best measured by using both precision and recall. See also [recall](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx2154357), [accuracy](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx3125742).\n\n\n\n\n\n private cloud \n\nA cloud computing environment in which access is limited to members of an enterprise and partner networks. See also [public cloud](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx4585370).\n\n\n\n\n\n private image repository \n\nThe combination of an organization's IBM Cloud registry and its namespace. The private image repository is used when referencing an image in a command. See also [image](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx2024928), [namespace](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx2031005).\n\n\n\n\n\n private key \n\nAn algorithmic pattern used to encrypt messages that only the corresponding public key can decrypt. The private key is also used to decrypt messages that were encrypted by the corresponding public key. The private key is kept on the user system and is protected by a password.\n\n\n\n\n\n private resource \n\nAn entry that is visible only to account owners and their included accounts. When resources are created, they are private by default.",
          "url": "https://cloud.ibm.com/docs/overview?topic=overview-glossary",
          "score": 13.922491,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770264
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16444-3168-4952",
          "text": "\nYou can run a pre-annotator on documents that were added to the ground truth as part of the current workspace. Annotations that were added to the documents, reviewed, accepted, and promoted to ground truth within the current workspace are not stripped out.\n\n\n\n\n\n Running multiple pre-annotators \n\nKnowledge Studio allows you to run multiple pre-annotators at once. First, you need to prepare the pre-annotation methods that you want to use. For more information, see the following sections.\n\n\n\n* [Dictionaries](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannot)\n* [Machine learning model](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannotsire)\n* [Rules-based model](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotationwks_preannotrule)\n\n\n\n\n\n Configuring the order of pre-annotators \n\nWhen multiple pre-annotators are used, the first annotation made to a span of text is saved for the results, even if other pre-annotators attempt to annotate the same span of text later in the order. This doesn't apply to human annotations, which are preserved regardless of pre-annotation order.\n\nFor example, consider the example text IBM Watson. If a dictionary that is first in the order labels IBM as an Organization entity type, a machine learning model that is second in the order can't annotate IBM Watson as a Software Brand entity type because that would override the earlier annotation made to IBM.\n\nYou can view the current order of pre-annotators in the Order column on the Machine Learning Model > Pre-annotation page. To change the order, complete the following steps.\n\n\n\n1. Click Order Settings.\n2.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-preannotation",
          "score": 13.828521,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724770271
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "pre annotation\n "
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "Yes, there are indeed several types of pre-annotators available, including Natural Language Understanding, Dictionaries, Machine learning model, and Rules-based model. These pre-annotators can be used individually or in combination, depending on the specific needs of your project.\nRegarding the use of different tools for conversational and technical documents, it ultimately depends on the nature of the documents and the goals of the annotation project. For instance, Natural Language Understanding might be more suitable for conversational documents, while Dictionaries or Rules-based models could be more appropriate for technical documents with specialized terminology. It's essential to evaluate the specific requirements of your project and choose the most suitable pre-annotators accordingly."
    },
    {
      "speaker": "user",
      "text": "corelation os 3 metrics",
      "timestamp": 1724772544,
      "enrichments": {
        "Question Type": [
          "Keyword",
          "Non-Question",
          "Summarization"
        ],
        "Multi-Turn": [
          "Follow-up"
        ],
        "Answerability": [
          "ANSWERABLE"
        ]
      }
    },
    {
      "speaker": "agent",
      "text": "It seems like you are asking about the 3 metrics of F1, Precision and Recall in analyzing machine learning model. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0.  Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model.   Recall is determined by the number of correctly labeled annotations divided by the number of annotations that should have been created. ",
      "timestamp": 1724772546,
      "contexts": [
        {
          "document_id": "ibmcld_10916-28690-30835",
          "text": "\nA measure of a test's accuracy that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values. An F1 score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n false negative \n\nAn answer or annotation that is correct, but was predicted to be incorrect.\n\n\n\n\n\n false positive \n\nAn answer or annotation that is incorrect, but was predicted to be correct.\n\n\n\n\n\n feature \n\nA data member or attribute of a type.\n\n\n\n\n\n feature code \n\nA code that is applied to free accounts to unlock extra product resources and capabilities.\n\n\n\n\n\n Federal Risk and Authorization Management Program (FedRAMP) \n\nA United States government program that provides a standardized, risk-based approach for the adoption and use of cloud services by the US federal government. FedRAMP empowers agencies to use modern cloud technologies with an emphasis on security and protection of federal information, and helps accelerate the adoption of secure cloud solutions.\n\n\n\n\n\n federate \n\nTo merge two or more entities. For example, a company's registered domain could be federated with an IBMid.\n\n\n\n\n\n FedRAMP \n\nSee [Federal Risk and Authorization Management Program](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx10109081).\n\n\n\n\n\n feed \n\nA piece of code that configures an external event source to fire trigger events. See also [action](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx2012974).\n\n\n\n\n\n file share \n\nIn the IBM Cloud environment, a persistent storage system where users store and share files. In IBM Cloud Kubernetes Service, users can mount Docker volumes on file shares.\n\n\n\n\n\n fire \n\nTo activate a trigger.\n\n\n\n\n\n Fleiss Kappa score \n\nA measure of how consistently the same annotation was applied by multiple human annotators across overlapping documents. The Fleiss Kappa score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n floating IP address \n\nA public, routable IP address that makes use of 1-to-1 network address translation (NAT) so that a server can communicate with the public internet and private subnet within a cloud environment.",
          "url": "https://cloud.ibm.com/docs/overview?topic=overview-glossary",
          "score": 21.9228,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773362
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16490-7-2216",
          "text": "\nAnalyzing machine learning model performance \n\nReview the annotations that were added by the trained model to determine whether any adjustments must be made to the model to improve its ability to find valid entity mentions, relation mentions, and coreferences in the documents.\n\n\n\n About this task \n\nYou can analyze performance by viewing a summary of statistics for entity types, relation types, and coreferenced mentions. You can also analyze statistics that are presented in a confusion matrix. The confusion matrix helps you compare the annotations added by the machine learning model to the annotations in ground truth.\n\nThe model statistics provide the following metrics:\n\n\n\n* F1 score\n\nA measurement that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0. See [Analyzing low F1 scores](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-mlevaluate-mllowf1).\n* Precision\n\nA measurement that specifies what fraction of the machine learning model's output was accurate when compared to the human annotator output. Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model. A precision score of 1.0 for entity type A means that every mention that was labeled as entity type A does indeed belong to that classification. A low precision score helps you identify places where the machine learning model created incorrect annotations. The score says nothing about how many other mentions that were labeled as entity type A by the human annotator were missed by the machine learning model; the recall score reflects that information. See [Analyzing low precision scores](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-mlevaluate-mllowp).\n* Recall\n\nA measurement that specifies how many mentions that should have been annotated by a given label were actually annotated with that label - the right mentions being those that human annotators identified in the same documents.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-ml",
          "score": 20.071053,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773359
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16425-7-2236",
          "text": "\nAnalyzing machine learning model performance \n\nReview the annotations that were added by the trained model to determine whether any adjustments must be made to the model to improve its ability to find valid entity mentions, relation mentions, and coreferences in the documents.\n\n\n\n About this task \n\nYou can analyze performance by viewing a summary of statistics for entity types, relation types, and coreferenced mentions. You can also analyze statistics that are presented in a confusion matrix. The confusion matrix helps you compare the annotations added by the machine learning model to the annotations in ground truth.\n\nThe model statistics provide the following metrics:\n\n\n\n* F1 score\n\nA measurement that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0. See [Analyzing low F1 scores](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-mlevaluate-mllowf1).\n* Precision\n\nA measurement that specifies what fraction of the machine learning model's output was accurate when compared to the human annotator output. Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model. A precision score of 1.0 for entity type A means that every mention that was labeled as entity type A does indeed belong to that classification. A low precision score helps you identify places where the machine learning model created incorrect annotations. The score says nothing about how many other mentions that were labeled as entity type A by the human annotator were missed by the machine learning model; the recall score reflects that information. See [Analyzing low precision scores](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-mlevaluate-mllowp).\n* Recall\n\nA measurement that specifies how many mentions that should have been annotated by a given label were actually annotated with that label - the right mentions being those that human annotators identified in the same documents.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
          "score": 19.861538,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773356
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16436-9264-11446",
          "text": "\nAn entity type exists independently and can be uniquely identified.\n\n\n\n\n\n\n\n F \n\n\n\n* F1 score\n\nA measure of a test's accuracy that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values. An F1 score reaches its best value at 1 and worst value at 0.\n* false negative\n\nAn answer or annotation that is correct, but was predicted to be incorrect.\n* false positive\n\nAn answer or annotation that is incorrect, but was predicted to be correct.\n* feature\n\nA data member or attribute of a type.\n* Fleiss Kappa score\n\nA measure of how consistently the same annotation was applied by multiple human annotators across overlapping documents. The Fleiss Kappa score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n\n\n G \n\n\n\n* ground truth\n\nThe set of vetted data, consisting of annotations added by human annotators, that is used to adapt a machine learning model to a particular domain. Ground truth is used to train machine learning models, measure model performance (precision and recall), and calculate headroom to decide where to focus development efforts for improving performance. Accuracy of ground truth is essential since inaccuracies in the ground truth will correlate to inaccuracies in the components that use it.\n\n\n\n\n\n\n\n H \n\n\n\n* headroom analysis\n\nThe process of determining how much improvement in accuracy, precision, or recall can be expected by addressing some class of problems that are identified while performing accuracy analysis.\n* human annotator\n\nA subject matter expert who reviews, modifies, and augments the results of pre-annotation by identifying mentions, entity type relationships, and mention coreferences. By examining text in context, a human annotator helps determine ground truth and improve the accuracy of the machine learning model.\n\n\n\n\n\n\n\n I \n\n\n\n* inter-annotator agreement\n\nA measure of how similarly a document in two or more document sets is annotated.\n\n\n\n\n\n\n\n K \n\n\n\n* knowledge graph\n\nA model that consolidates typed entities, their relationships, their properties, and hierarchical taxonomies to represent an organization of concepts for a given domain.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-glossary",
          "score": 18.479345,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773352
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16425-20985-23039",
          "text": "\nA precision score reaches its best value at 1 and worst value at 0. A low precision score indicates that the machine learning model generated incorrect annotations.\n\n\n\n\n\n Causes \n\nLow precision scores can occur for many different reasons that depend on the domain, type system complexity, appropriateness of training documents, human annotator skills, and other factors.\n\n\n\n\n\n Resolving the problem \n\nTune the performance of your machine learning model by performing one or more of following steps then retrain your model:\n\n\n\n1. Identify commonly occurring types with low precision.\n2. Identify commonly confused types. This information can be found by looking at the numbers that are off the diagonal in the confusion matrix.\n3. Review errors where the machine learning model has high confidence.\n4. Find patterns in the false negatives in the confusion matrix.\n5. If certain types have low precision scores, review the clarity of the annotation guidelines that apply to those types.\n\n\n\n![How to resolve low precision scores.](https://cloud.ibm.com/docs-content/v1/content/148d3bd95f946aa1bb53ea1540475f522e6b61c9/watson-knowledge-studio-data/images/wks_lowprec.svg)\n\nFigure 2. How to resolve low precision scores\n\n\n\n\n\n\n\n Analyzing low recall scores \n\nTune the performance of your machine learning model to address low recall scores. At a high level, low recall indicates a need to add more training data.\n\n\n\n Symptoms \n\nA recall score reaches its best value at 1 and worst value at 0. A low recall score indicates that the machine learning model failed to create annotations that it should have created.\n\n\n\n\n\n Causes \n\nLow recall scores can occur for many different reasons that depend on the domain, type system complexity, appropriateness of training documents, human annotator skills, and other factors.\n\n\n\n\n\n Resolving the problem \n\nTune the performance of your machine learning model by performing one or more of following steps then retrain your model:\n\n\n\n1. Identify commonly occurring types with low recall.\n2. Identify commonly confused types.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
          "score": 15.766418,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773350
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        },
        {
          "document_id": "ibmcld_16493-15394-17165",
          "text": "\nRecall, which is a measure of sensitivity, is determined by the number of correct positive results divided by the number of positive results that should have been returned. Accuracy is best measured by using both precision and recall. See also [accuracy](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossarygloss_A) and [precision](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossarygloss_P).\n* relation\n\nTypically a verb that reflects how entities are related to one another. For example, \"lives in\" is a relation between a person and a town. A relation links two different entities in the same sentence.\n* relation type\n\nA binary, uni-directional relationship between two entities. For example, MaryemployedBy IBM is a valid relationship; IBM employedByMary is not.\n* role\n\nAn attribute that provides a context-sensitive meaning of a mention. For example, in the phrase \"I went to IBM today\", IBM is the mention, Organization is the entity type, and Facility is the role of the entity type.\n* rule set\n\nA set of rules that define patterns for annotating text. If a pattern applies, then the actions of the rule are performed on the matched annotations. A rule typically specifies the condition that must match, an optional quantifier, a list of additional constraints that the matched text must fulfill, and the actions to be taken when a match occurs, such as creating a new annotation or modifying an existing annotation.\n\n\n\n\n\n\n\n S \n\n\n\n* subtype\n\nA type that extends or implements another type; the supertype.\n* surface form\n\nThe form of a word or multiword unit as it is found in the corpus. For example, some surface forms of the lemma 'organize' are the terms 'organizing' and 'organized'.",
          "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossary",
          "score": 15.590508,
          "feedback": {
            "relevant": {
              "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
                "value": "yes",
                "timestamp": 1724773347
              }
            }
          },
          "query": {
            "query": {
              "bool": {
                "must": {
                  "text_expansion": {
                    "ml.tokens": {
                      "model_id": ".elser_model_1",
                      "model_text": "F1 recall precision\n "
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "original_text": "It seems like you are asking about the evaluation metrics for coreference resolution. The three main metrics used for evaluating coreference resolution are MUC (Metric for Unified Coreference Evaluation), B-CUBED (B-Cubed F-Score), and CEAF (Coreference Entity Average F-Score). These metrics evaluate the quality of the coreference chains produced by a model by comparing them to the gold standard annotations."
    }
  ],
  "status": "accepted",
  "status_history": [
    {
      "author": "f6ce971c-68d2-4c26-b1c0-477dd28a5181",
      "status": "created",
      "timestamp": 1724773681
    },
    {
      "author": "868c6e58-57c1-48ee-8c67-832dbedcac18",
      "status": "accepted",
      "timestamp": 1725541581
    },
    {
      "author": "7ee1aecf-492d-4847-a436-3db915c57d2e",
      "status": "edited",
      "timestamp": 1726582227
    },
    {
      "author": "7ee1aecf-492d-4847-a436-3db915c57d2e",
      "status": "accepted",
      "timestamp": 1726582882
    }
  ],
  "editor": "7ee1aecf-492d-4847-a436-3db915c57d2e",
  "domain": "mt-rag-ibmcloud-elser-512-100-20240502",
  "reviewer": "7ee1aecf-492d-4847-a436-3db915c57d2e"
}