{
  "task_id": "ddbbbe7ea13560c5768639207e1ca604<::>7",
  "conversation_id": "38",
  "turn_id": 7,
  "domain": "cloud",
  "input": "corelation os 3 metrics",
  "gold_answer": "It seems like you are asking about the 3 metrics of F1, Precision and Recall in analyzing machine learning model. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0.  Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model.   Recall is determined by the number of correctly labeled annotations divided by the number of annotations that should have been created. ",
  "enrichments": {
    "Question Type": [
      "Keyword",
      "Non-Question",
      "Summarization"
    ],
    "Multi-Turn": [
      "Follow-up"
    ],
    "Answerability": [
      "ANSWERABLE"
    ]
  },
  "retrieved_contexts": [
    {
      "document_id": "ibmcld_10916-28690-30835",
      "text": "\nA measure of a test's accuracy that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values. An F1 score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n false negative \n\nAn answer or annotation that is correct, but was predicted to be incorrect.\n\n\n\n\n\n false positive \n\nAn answer or annotation that is incorrect, but was predicted to be correct.\n\n\n\n\n\n feature \n\nA data member or attribute of a type.\n\n\n\n\n\n feature code \n\nA code that is applied to free accounts to unlock extra product resources and capabilities.\n\n\n\n\n\n Federal Risk and Authorization Management Program (FedRAMP) \n\nA United States government program that provides a standardized, risk-based approach for the adoption and use of cloud services by the US federal government. FedRAMP empowers agencies to use modern cloud technologies with an emphasis on security and protection of federal information, and helps accelerate the adoption of secure cloud solutions.\n\n\n\n\n\n federate \n\nTo merge two or more entities. For example, a company's registered domain could be federated with an IBMid.\n\n\n\n\n\n FedRAMP \n\nSee [Federal Risk and Authorization Management Program](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx10109081).\n\n\n\n\n\n feed \n\nA piece of code that configures an external event source to fire trigger events. See also [action](https://cloud.ibm.com/docs/overview?topic=overview-glossaryx2012974).\n\n\n\n\n\n file share \n\nIn the IBM Cloud environment, a persistent storage system where users store and share files. In IBM Cloud Kubernetes Service, users can mount Docker volumes on file shares.\n\n\n\n\n\n fire \n\nTo activate a trigger.\n\n\n\n\n\n Fleiss Kappa score \n\nA measure of how consistently the same annotation was applied by multiple human annotators across overlapping documents. The Fleiss Kappa score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n floating IP address \n\nA public, routable IP address that makes use of 1-to-1 network address translation (NAT) so that a server can communicate with the public internet and private subnet within a cloud environment.",
      "url": "https://cloud.ibm.com/docs/overview?topic=overview-glossary",
      "score": 21.9228,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773362
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16490-7-2216",
      "text": "\nAnalyzing machine learning model performance \n\nReview the annotations that were added by the trained model to determine whether any adjustments must be made to the model to improve its ability to find valid entity mentions, relation mentions, and coreferences in the documents.\n\n\n\n About this task \n\nYou can analyze performance by viewing a summary of statistics for entity types, relation types, and coreferenced mentions. You can also analyze statistics that are presented in a confusion matrix. The confusion matrix helps you compare the annotations added by the machine learning model to the annotations in ground truth.\n\nThe model statistics provide the following metrics:\n\n\n\n* F1 score\n\nA measurement that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0. See [Analyzing low F1 scores](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-mlevaluate-mllowf1).\n* Precision\n\nA measurement that specifies what fraction of the machine learning model's output was accurate when compared to the human annotator output. Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model. A precision score of 1.0 for entity type A means that every mention that was labeled as entity type A does indeed belong to that classification. A low precision score helps you identify places where the machine learning model created incorrect annotations. The score says nothing about how many other mentions that were labeled as entity type A by the human annotator were missed by the machine learning model; the recall score reflects that information. See [Analyzing low precision scores](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-mlevaluate-mllowp).\n* Recall\n\nA measurement that specifies how many mentions that should have been annotated by a given label were actually annotated with that label - the right mentions being those that human annotators identified in the same documents.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-evaluate-ml",
      "score": 20.071053,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773359
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16425-7-2236",
      "text": "\nAnalyzing machine learning model performance \n\nReview the annotations that were added by the trained model to determine whether any adjustments must be made to the model to improve its ability to find valid entity mentions, relation mentions, and coreferences in the documents.\n\n\n\n About this task \n\nYou can analyze performance by viewing a summary of statistics for entity types, relation types, and coreferenced mentions. You can also analyze statistics that are presented in a confusion matrix. The confusion matrix helps you compare the annotations added by the machine learning model to the annotations in ground truth.\n\nThe model statistics provide the following metrics:\n\n\n\n* F1 score\n\nA measurement that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values, where an F1 score reaches its best value at 1 and worst value at 0. See [Analyzing low F1 scores](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-mlevaluate-mllowf1).\n* Precision\n\nA measurement that specifies what fraction of the machine learning model's output was accurate when compared to the human annotator output. Precision is determined by the number of correctly labeled annotations divided by the total number of annotations added by the machine learning model. A precision score of 1.0 for entity type A means that every mention that was labeled as entity type A does indeed belong to that classification. A low precision score helps you identify places where the machine learning model created incorrect annotations. The score says nothing about how many other mentions that were labeled as entity type A by the human annotator were missed by the machine learning model; the recall score reflects that information. See [Analyzing low precision scores](https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-mlevaluate-mllowp).\n* Recall\n\nA measurement that specifies how many mentions that should have been annotated by a given label were actually annotated with that label - the right mentions being those that human annotators identified in the same documents.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
      "score": 19.861538,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773356
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16436-9264-11446",
      "text": "\nAn entity type exists independently and can be uniquely identified.\n\n\n\n\n\n\n\n F \n\n\n\n* F1 score\n\nA measure of a test's accuracy that considers both precision and recall to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall values. An F1 score reaches its best value at 1 and worst value at 0.\n* false negative\n\nAn answer or annotation that is correct, but was predicted to be incorrect.\n* false positive\n\nAn answer or annotation that is incorrect, but was predicted to be correct.\n* feature\n\nA data member or attribute of a type.\n* Fleiss Kappa score\n\nA measure of how consistently the same annotation was applied by multiple human annotators across overlapping documents. The Fleiss Kappa score reaches its best value at 1 and worst value at 0.\n\n\n\n\n\n\n\n G \n\n\n\n* ground truth\n\nThe set of vetted data, consisting of annotations added by human annotators, that is used to adapt a machine learning model to a particular domain. Ground truth is used to train machine learning models, measure model performance (precision and recall), and calculate headroom to decide where to focus development efforts for improving performance. Accuracy of ground truth is essential since inaccuracies in the ground truth will correlate to inaccuracies in the components that use it.\n\n\n\n\n\n\n\n H \n\n\n\n* headroom analysis\n\nThe process of determining how much improvement in accuracy, precision, or recall can be expected by addressing some class of problems that are identified while performing accuracy analysis.\n* human annotator\n\nA subject matter expert who reviews, modifies, and augments the results of pre-annotation by identifying mentions, entity type relationships, and mention coreferences. By examining text in context, a human annotator helps determine ground truth and improve the accuracy of the machine learning model.\n\n\n\n\n\n\n\n I \n\n\n\n* inter-annotator agreement\n\nA measure of how similarly a document in two or more document sets is annotated.\n\n\n\n\n\n\n\n K \n\n\n\n* knowledge graph\n\nA model that consolidates typed entities, their relationships, their properties, and hierarchical taxonomies to represent an organization of concepts for a given domain.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-glossary",
      "score": 18.479345,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773352
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16425-20985-23039",
      "text": "\nA precision score reaches its best value at 1 and worst value at 0. A low precision score indicates that the machine learning model generated incorrect annotations.\n\n\n\n\n\n Causes \n\nLow precision scores can occur for many different reasons that depend on the domain, type system complexity, appropriateness of training documents, human annotator skills, and other factors.\n\n\n\n\n\n Resolving the problem \n\nTune the performance of your machine learning model by performing one or more of following steps then retrain your model:\n\n\n\n1. Identify commonly occurring types with low precision.\n2. Identify commonly confused types. This information can be found by looking at the numbers that are off the diagonal in the confusion matrix.\n3. Review errors where the machine learning model has high confidence.\n4. Find patterns in the false negatives in the confusion matrix.\n5. If certain types have low precision scores, review the clarity of the annotation guidelines that apply to those types.\n\n\n\n![How to resolve low precision scores.](https://cloud.ibm.com/docs-content/v1/content/148d3bd95f946aa1bb53ea1540475f522e6b61c9/watson-knowledge-studio-data/images/wks_lowprec.svg)\n\nFigure 2. How to resolve low precision scores\n\n\n\n\n\n\n\n Analyzing low recall scores \n\nTune the performance of your machine learning model to address low recall scores. At a high level, low recall indicates a need to add more training data.\n\n\n\n Symptoms \n\nA recall score reaches its best value at 1 and worst value at 0. A low recall score indicates that the machine learning model failed to create annotations that it should have created.\n\n\n\n\n\n Causes \n\nLow recall scores can occur for many different reasons that depend on the domain, type system complexity, appropriateness of training documents, human annotator skills, and other factors.\n\n\n\n\n\n Resolving the problem \n\nTune the performance of your machine learning model by performing one or more of following steps then retrain your model:\n\n\n\n1. Identify commonly occurring types with low recall.\n2. Identify commonly confused types.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
      "score": 15.766418,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773350
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16493-15394-17165",
      "text": "\nRecall, which is a measure of sensitivity, is determined by the number of correct positive results divided by the number of positive results that should have been returned. Accuracy is best measured by using both precision and recall. See also [accuracy](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossarygloss_A) and [precision](https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossarygloss_P).\n* relation\n\nTypically a verb that reflects how entities are related to one another. For example, \"lives in\" is a relation between a person and a town. A relation links two different entities in the same sentence.\n* relation type\n\nA binary, uni-directional relationship between two entities. For example, MaryemployedBy IBM is a valid relationship; IBM employedByMary is not.\n* role\n\nAn attribute that provides a context-sensitive meaning of a mention. For example, in the phrase \"I went to IBM today\", IBM is the mention, Organization is the entity type, and Facility is the role of the entity type.\n* rule set\n\nA set of rules that define patterns for annotating text. If a pattern applies, then the actions of the rule are performed on the matched annotations. A rule typically specifies the condition that must match, an optional quantifier, a list of additional constraints that the matched text must fulfill, and the actions to be taken when a match occurs, such as creating a new annotation or modifying an existing annotation.\n\n\n\n\n\n\n\n S \n\n\n\n* subtype\n\nA type that extends or implements another type; the supertype.\n* surface form\n\nThe form of a word or multiword unit as it is found in the corpus. For example, some surface forms of the lemma 'organize' are the terms 'organizing' and 'organized'.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-glossary",
      "score": 15.590508,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724773347
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "F1 recall precision\n "
                }
              }
            }
          }
        }
      }
    }
  ],
  "timestamp": 1724772544
}