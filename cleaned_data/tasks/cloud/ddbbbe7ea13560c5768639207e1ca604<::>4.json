{
  "task_id": "ddbbbe7ea13560c5768639207e1ca604<::>4",
  "conversation_id": "38",
  "turn_id": 4,
  "domain": "cloud",
  "input": "So, 3 ppl at least?",
  "gold_answer": "I am not sure what you are asking. The minimal number of annotators needed is two, as per my answer to your previous question above. There can of course be more than two annotators depending on the project, but the minimal requirement is two annotators.",
  "enrichments": {
    "Multi-Turn": [
      "Clarification"
    ],
    "Question Type": [
      "Factoid"
    ],
    "Answerability": [
      "UNANSWERABLE"
    ]
  },
  "retrieved_contexts": [
    {
      "document_id": "ibmcld_16410-8324-10312",
      "text": "\nInter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set.\n\nYou cannot change the annotation set name after the set is created.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.\n* Open a task to add annotation sets to it. Ensure that the annotation sets that you add include documents that overlap with documents in the original annotation sets.\n\n\n\nFrom the Settings tab of the main navigation, you can specify the following information:\n\n\n\n* Specify preferences for using colors and keyboard shortcuts in the ground truth editor.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documents",
      "score": 41.94719,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724767083
          },
          "7ee1aecf-492d-4847-a436-3db915c57d2e": {
            "value": "no",
            "timestamp": 1726508452
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16468-8522-10510",
      "text": "\nIf no annotation sets are available, click Create Annotation Sets.\n\n\n\n1. For the base set, select the document set or annotation set that you want to divide into annotation sets.\n2. For the overlap value, specify the percentage of documents that you want to include in each annotation set. Inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set. You cannot change the annotation set name after the set is created.\n\nThe maximum size of the annotation set name is 256 characters.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio?topic=watson-knowledge-studio-annotate-documents",
      "score": 38.855537,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724767086
          },
          "7ee1aecf-492d-4847-a436-3db915c57d2e": {
            "value": "no",
            "timestamp": 1726508476
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_16417-7044-9159",
      "text": "\nYou can see which documents are in an annotation set by opening the Assets> Documents page, clicking the Annotation Sets tab, and then clicking the names of the sets.\n\nYou might experience situations in which no overlapping documents are found. This might happen, for example, if you create annotation sets in two rounds and add them to the same task. Even though the annotation sets were created at about the same time, they don't have any documents in common. For another example, if you create annotation sets with overlapping documents, but add one annotation set per task instead of adding all of the annotation sets to a single task, no overlapping documents will be found and inter-annotator agreement cannot be calculated.\n\n\n\n\n\n Procedure \n\nTo assess annotation agreement between human annotators:\n\n\n\n1. Log in as a Knowledge Studio administrator or project manager, and select your workspace.\n2. Select the Machine Learning Model > Annotations page, then click the Annotation Tasks tab. Open the task that you want to evaluate.\n3. Click Calculate Inter-Annotator Agreement. The default view shows agreement scores for how consistently pairs of human annotators annotated mentions. The top row shows the overall consistency between each pair of annotators, and the table below shows how consistently a pair of annotators labeled specific mentions in the text.\n4. To explore how consistently pairs of human annotators annotated relations and coreferences, select Relation or Coreference from the first menu.\n5. To explore how consistently a pair of human annotators annotated entities, relations, or coreferences in specific overlapping documents, select Document in the second menu and then select the pair of annotators that you want to evaluate.\n6. After reviewing the scores, you can decide whether you want to approve or reject annotation sets that are in Submitted status. After an annotation set is submitted, a check box is displayed next to its name. Take one of these actions:\n\n\n\n* If the inter-annotator agreement scores are acceptable for an annotation set, select the check box and click Accept.",
      "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-build-groundtruth",
      "score": 36.50021,
      "feedback": {
        "relevant": {
          "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
            "value": "yes",
            "timestamp": 1724767089
          },
          "7ee1aecf-492d-4847-a436-3db915c57d2e": {
            "value": "no",
            "timestamp": 1726508495
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?\n|assistant|: The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents.   For example, if you specify a 20% overlap value for a corpus that contains 30 documents, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators.  \n|user|: data volume\n|assistant|: There are certain guidelines for the volume of data when adding documents for annotation to train a machine learning model. It is recommended to provide a set of documents that have a total size of about 300,000 words. The number of words can be adjusted based on the complexity of the type system. Each document should be limited to a page or two of content, preferably fewer than 2,000 words and closer to 1,000 words per document.  A goal to aim for is to eventually have at least 50 annotations for each entity type and 50 for each relation type in the document collection.  The set of documents for training must contain at least 10 annotated documents.\n|user|: So, 3 ppl at least?"
                }
              }
            }
          }
        }
      }
    }
  ],
  "timestamp": 1724766763
}