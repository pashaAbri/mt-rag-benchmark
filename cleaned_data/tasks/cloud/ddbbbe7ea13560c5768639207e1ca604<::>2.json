{
  "task_id": "ddbbbe7ea13560c5768639207e1ca604<::>2",
  "conversation_id": "38",
  "turn_id": 2,
  "domain": "cloud",
  "user": {
    "speaker": "user",
    "text": "How many annotators are needed?",
    "timestamp": 1724759855,
    "enrichments": {
      "Question Type": [
        "Explanation"
      ],
      "Multi-Turn": [
        "Follow-up"
      ],
      "Answerability": [
        "ANSWERABLE"
      ]
    }
  },
  "agent": {
    "speaker": "agent",
    "text": "The number of annotators needed can vary depending on the specific requirements of the project. However, inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. That allows you to view the scores by comparing a pair of human annotators (for example, compare all of John's mention annotations to all of Mary's mention annotations). You can also view the scores by comparing specific documents (for example, compare the relation annotations that John made in one document to the relation annotations that Mary made in the same document).",
    "timestamp": 1724759858,
    "contexts": [
      {
        "document_id": "ibmcld_16425-14122-16407",
        "text": "\nAnother indicator of annotation inconsistency is if you have enough annotations, but the corpus density is low. Density can be impacted when a mention that is significant in the domain literature occurs often, but is annotated as different types across the document set.\n\nLow precision is often an indication that you need to improve annotation consistency. To do so, review the annotation guidelines, better train the human annotators, and ensure that human annotators are working in concert and not in isolation from one another.\n\nCheck the inter-annotator agreement score. This score, which measures the degree of agreement between different annotators' output on the same document, is a valuable number. Not only does this score tell you the quality of the ground truth documents that will be used to train the machine learning model, but it also indicates the upper bound of machine learning model performance. A model that is trained on these documents is unlikely to outperform the best agreement that humans can reach. For example, if performance persists at 75 and does not go higher, take a look at the inter-annotator agreement results. If the inter-annotator agreement score is 80, take actions to better train the human annotators and ensure that conflicts are correctly resolved (according to the annotation guidelines) during adjudication. If humans can't agree on how something should be labeled, then it's unlikely that a machine learning model will apply the correct labels.\n* Enhance human annotator guidelines\n\nClear and comprehensive annotator guidelines are a crucial part of a harmonious and successful annotation development effort. Human annotators have a tough job to do. There can be nuances in assigning entity and relation types that are hard to anticipate until you start to work with the domain documents. The guidelines can provide a sanity check to human annotators as they evaluate documents. The guidelines should be a living and changing document, especially at the beginning of the annotation process. They provide a key feedback loop because a human annotator can capture things she learned while annotating a few documents, then as she or someone else annotates a few more documents, new tips and gotchas can be added to the guideline, and so on.",
        "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-evaluate-ml",
        "score": 23.490267,
        "feedback": {
          "relevant": {
            "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
              "value": "yes",
              "timestamp": 1724761996
            },
            "7ee1aecf-492d-4847-a436-3db915c57d2e": {
              "value": "no",
              "timestamp": 1726507432
            }
          }
        },
        "query": {
          "query": {
            "bool": {
              "must": {
                "text_expansion": {
                  "ml.tokens": {
                    "model_id": ".elser_model_1",
                    "model_text": "|user|: What is the Score Agreement level if it is less than 0?\n|assistant|: If the Score Agreement level is less than 0, it indicates poor agreement among the annotators. This can suggest that the annotation guidelines or instructions may need to be revised to improve consistency.\n|user|: How many annotators are needed?"
                  }
                }
              }
            }
          }
        }
      },
      {
        "document_id": "ibmcld_16410-8324-10312",
        "text": "\nInter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents. For example, if you specify a 20% overlap value for a corpus that contains 30 documents, and you divide the corpus into 3 document sets, 6 documents (20%) will be annotated by all human annotators. The remaining 24 documents will be divided among the 3 human annotators (8 each). Thus, each annotator receives 14 documents to annotate (6+8).\n\n\n\nAn annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\n\n\n\n1. Select a user name from the list of human annotators.\n2. Name the annotation set.\n\nAs a good practice for evaluating a human annotator's work as the workspace progresses, you might want to create annotation set names that identify the human annotator assigned to the set.\n\nYou cannot change the annotation set name after the set is created.\n3. Click Generate.\n\n\n\n6. A list of available annotation sets is displayed under Available Sets, along with the names of the human annotators assigned to them. To add available sets to your annotation task, click Add to task.\n7. Make sure that all of the annotation sets that you want to include in the task appear under Selected Sets, then click Save to create the task.\n\n\n\n\n\n\n\n What to do next \n\nAfter the task is created, you can return to the Annotation Tasks tab on the Machine Learning Model > Annotations page to view the progress of each human annotator. Also, you can complete the following tasks:\n\n\n\n* Check approved documents that overlap between annotation sets to resolve annotation conflicts.\n* Open a task to add annotation sets to it. Ensure that the annotation sets that you add include documents that overlap with documents in the original annotation sets.\n\n\n\nFrom the Settings tab of the main navigation, you can specify the following information:\n\n\n\n* Specify preferences for using colors and keyboard shortcuts in the ground truth editor.",
        "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-annotate-documents",
        "score": 16.722137,
        "feedback": {
          "relevant": {
            "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
              "value": "yes",
              "timestamp": 1724761994
            }
          }
        },
        "query": {
          "query": {
            "bool": {
              "must": {
                "text_expansion": {
                  "ml.tokens": {
                    "model_id": ".elser_model_1",
                    "model_text": "inter annotator agreement"
                  }
                }
              }
            }
          }
        }
      },
      {
        "document_id": "ibmcld_16417-1764-4158",
        "text": "\nAs the project manager resolves annotation conflicts, the approved annotations are promoted to ground truth.\n\n\n\nKeep in mind that human annotation always requires judgment calls. The annotation guidelines can do a lot to ensure that text is correctly and consistently annotated, but even the best guidelines are open to interpretation by humans. To obtain ground truth, you will want to spend time training and educating human annotators so that they can make the best judgments when analyzing domain content.\n\n\n\n Inter-annotator agreement \n\nAfter human annotators have annotated the documents, you must determine which documents to promote to ground truth. Start by reviewing the inter-annotator agreement scores. Documents with low scores are candidates to be rejected and returned to the human annotator for improvement.\n\nWhen calculating inter-annotator agreement scores, the system examines all overlapping documents in all annotation sets in the task, regardless of the status of the sets. Because you cannot accept or reject annotation sets until they are in the Submitted status, you might not want to evaluate inter-annotator agreement until all annotation sets are submitted, or you might want to limit your review to only the human annotators who submitted their completed annotation sets.\n\nThe inter-annotator agreement scores show how different human annotators annotated mentions, relations, and coreference chains. You can view the scores by comparing a pair of human annotators (for example, compare all of John's mention annotations to all of Mary's mention annotations). You can also view the scores by comparing specific documents (for example, compare the relation annotations that John made in one document to the relation annotations that Mary made in the same document).\n\nTo help you identify areas that require investigation, scores that fall below the value that you specified for the inter-annotator agreement threshold are highlighted in red. In early stages of your annotation project, you might find that relation scores are often worse than mention scores because a perfect relation annotation requires that the mentions that define the relationship be in agreement first.\n\nThe score in the All column is a Fleiss Kappa score. It represents how consistently the same annotation was applied by multiple human annotators across all overlapping documents in the task.",
        "url": "https://cloud.ibm.com/docs/watson-knowledge-studio-data?topic=watson-knowledge-studio-data-build-groundtruth",
        "score": 17.094318,
        "feedback": {
          "relevant": {
            "f6ce971c-68d2-4c26-b1c0-477dd28a5181": {
              "value": "yes",
              "timestamp": 1724761991
            }
          }
        },
        "query": {
          "query": {
            "bool": {
              "must": {
                "text_expansion": {
                  "ml.tokens": {
                    "model_id": ".elser_model_1",
                    "model_text": "inter annotator agreement"
                  }
                }
              }
            }
          }
        }
      }
    ],
    "original_text": "The number of annotators needed can vary depending on the specific requirements of the project. However, to calculate inter-annotator agreement scores, at least two human annotators are required. Additionally, an annotation set that you plan to use to train a machine learning model must contain at least 10 annotated documents.\nIt's also important to note that an annotation set that will be used to train a machine learning model must contain at least 10 annotated documents, and inter-annotator agreement scores cannot be calculated unless two or more human annotators annotate the same documents."
  }
}