{
  "task_id": "17_7",
  "conversation_id": "17",
  "turn_id": 7,
  "domain": "cloud",
  "input": "Worker node",
  "gold_answer": " If you are asking about worker nodes With IBM Cloud Kubernetes Service, they are instances managed by your cluster through virtual machines. Those worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers .You are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Nevertheless the underlying hardware is shared with other IBM customers. Worker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software Additionally worker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. ",
  "enrichments": {
    "Question Type": [
      "Keyword",
      "Summarization"
    ],
    "Multi-Turn": [
      "Follow-up"
    ],
    "Answerability": [
      "ANSWERABLE"
    ]
  },
  "retrieved_contexts": [
    {
      "document_id": "ibmcld_06160-11142-12906",
      "text": "\nIf the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\nkubectl top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%\n2.",
      "url": "https://cloud.ibm.com/docs/containers?topic=containers-ts-critical-notready",
      "score": 50.234882,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "no",
            "timestamp": 1724675937
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_10596-11475-13230",
      "text": "\n5. If the issue is still not resolved, follow the steps to [gather your worker node data](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready-gather) and open a support ticket.\n\n\n\n\n\n\n\n\n\n Gathering data for a support case \n\nIf you are unable to resolve the issue with the troubleshooting steps, gather information about your worker nodes. Then, [open a support ticket](https://cloud.ibm.com/unifiedsupport/cases/form) and include the worker node information you gathered.\n\nBefore you open a support ticket, review the information and follow any troubleshooting steps in [Debugging worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-debug_worker_nodes), [Worker node states](https://cloud.ibm.com/docs/containers?topic=containers-worker-node-state-reference), and [Troubleshooting worker nodes in Critical or NotReady state](https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notreadyts-critical-notready).\n\nIf all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.\n\n\n\n Before you begin \n\nCheck the conditions of your worker nodes and cluster before you gather data.\n\n\n\n1. Check the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.\n\noc top node\n\nExample output.\n\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\n10.001.1.01 640m 16% 6194Mi 47%\n10.002.2.02 2686m 68% 4024Mi 30%\n10.003.3.03 2088m 53% 10735Mi 81%",
      "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-ts-critical-notready",
      "score": 49.99304,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "no",
            "timestamp": 1724674377
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_10394-7-1848",
      "text": "\nUpdating Classic worker nodes that use OpenShift Data Foundation \n\nClassic infrastructure\n\nFor Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.\n\nThe following tutorial covers both major and minor worker node updates. Each step is flagged with\n\nMajor\n\nor\n\nMinor\n\n.\n\n\n\nMajor\n\nApplies to major updates, for example if you are updating your worker nodes to a new major version, such as from 4.11 to 4.12 as well as OpenShift Data Foundation from 4.11 to 4.12\nMinor\n\nApplies to minor patch updates, for example if you are updating from 4.12.15_1542_openshift to 4.12.16_1544_openshift while keeping OpenShift Data Foundation at version 4.12.\n\n\n\nSkipping versions during an upgrade, such as from 4.8 to 4.12 is not supported.\n\n[Log in to your account. If applicable, target the appropriate resource group. Set the context for your cluster.](https://cloud.ibm.com/docs/containers?topic=containers-access_cluster)\n\nBefore updating your worker nodes, make sure to back up your app data. Also, plan to complete the following steps for one worker node at a time. Repeat the steps for each worker node that you want to update.\n\n\n\n Step 1: Update the cluster master \n\nMajor\n\n\n\n1. If you are updating your worker nodes to a new major version, such as from 4.11 to 4.12, update the cluster master first.\n\nibmcloud oc cluster master update --cluster CLUSTER [--version MAJOR.MINOR.PATCH] [--force-update] [-f] [-q]\n\nExample command:\n\nibmcloud oc cluster master update --cluster mycluster --version 4.11.42 --force-update\n2. Wait until the master update finishes.",
      "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-storage-update-classic",
      "score": 49.552605,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "yes",
            "timestamp": 1724674371
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "|user|: What are the steps to be taken to gather the relevant worker node data?\n|assistant|:    There are general steps to gather the relevant worker node data: First,  Check the conditions of your worker nodes and cluster before you gather data. This includes checking the CPU and memory level of your nodes. If any node is over 80% in either CPU or memory usage, consider provisioning more nodes or reducing your workload.Secondly if all worker nodes in a cluster, or in a single zone, subnet, or VLAN are affected, you can open an initial support ticket without gathering data. However, you might later be asked to gather the relevant data. If only one or some of your worker nodes are affected, you must gather the relevant data to include in your support ticket.  Before opening a support ticket, it is recommended to review the information and follow any troubleshooting steps in Debugging worker nodes, Worker node states, and Troubleshooting worker nodes in Critical or NotReady state. To gather worker node data, you can use the command 'kubectl top node' (for Kubernetes clusters) or 'oc top node' (for OpenShift clusters), which will display the CPU and memory usage of each node and once you have gathered this information, include it in your support ticket when you open one.                        \n|user|: How can i update a classic worker node?\n|assistant|: An update is available for your worker nodes in a classic infrastructure cluster. As security updates and patches are put in place for the API server and other master components, you must be sure that the worker nodes remain in sync. You can make two types of updates: updating only the patch version, or updating the major.minor version with the patch version. For Classic clusters with a storage solution such as OpenShift Data Foundation you must cordon, drain, and replace each worker node sequentially. If you deployed OpenShift Data Foundation to a subset of worker nodes in your cluster, then after you replace the worker node, you must then edit the ocscluster resource to include the new worker node.You can update the master Kubernetes version with the ibmcloud ks cluster master updatecommand. Remember that your worker nodes can be only up to two versions behind the master version (n-2). The worker node IP address remains the same after the update operation.\n|user|: Major. menor update.\n|assistant|:  To determine the type of update that is available, compare your current worker node versions to the latest worker node fix pack version in the Red Hat OpenShift version change log. Major updates are indicated by the first digit in the version label (4.x.x), minor updates are indicated by the second digit (x.7.x) and patch updates are indicated by the trailing digits (x.x.23_1528_openshift).Major.minor: A major.minor update moves up the Kubernetes version of the worker node to the same version as the master. This type of update often includes changes to the Kubernetes API or other behaviors that you must prepare your cluster for. Remember that your worker nodes can only be one version behind the master version (n-1). You can update the classic worker node to the same patch by using the ibmcloud oc worker update command.             \n|user|: parts of a tag.\n|assistant|: A tag consists of multiple parts as X.Y.Z-.[hash] where X represents the major version of an image; Y represents the minor version of an image and Z represents an incremental ID that determines the latest patched minor version.Depending on the tag that you use, you must consider upgrading the logging agent image in your DevOps maintenance plan, to resolve vulnerabilities and apply agent enhancements and agent bug fixes. Moreover  Tags are case-sensitive, and the maximum length of a tag is 128 characters.  The characters that are permitted to name tags are A-Z, 0-9, spaces, underscore, hyphen, period, and colon. Colons turn the tag into a string where you can isolate two logical parts, like a key:value pair. You can't use a colon in a tag without creating this pairing. A comma separate tags and can't be used within the tag name itself. If you add PII information in the name, you might be disclosing sensitive data to others in the same account.When you define your tags, do not add sensitive information in the tag name.Tags are visible to all members of an account.To control tag visibility, circulate tagging guidelines, and let users know that tags are visible account-wide.  Define the log ingestion strategy. Additionally  Ttags are accessible throughout an instance with the proper permissions. While the true organizational power of using tags as an organizational principle scales with you, you can access tags on an individual basis as well.\n|user|: node data\n|assistant|: I am not sure what you would like to know about node data but I can inform you that there are  some sort of nodes   For instance  ordering node  that collects transactions from network members, orders the transactions and bundles them into blocks. Also known as orderer. Another one is Peer that is a blockchain network resource that provides the services to execute and validate transactions, and maintain ledgers. The peer runs smart contract and is the holder of transaction history and the current state of assets on ledgers. They are owned and managed by organizations and are joined to channels.  Additionally to those two, CA is another node that  issues certificates to all network participants (peers, ordering services, clients, admins, and so on). These certificates, which include a signing certificate and private key, allow network participants to communicate, authenticate, and ultimately transact. These CAs will create all of the identities and certificates that belong to your organization, in addition to defining the organization itself. You can then use those identities to deploy nodes, create admin identities, and submit transactions.                   \n|user|: NodeSync\n|assistant|: NodeSync  is a continuous repair service that runs automatically in the background to validate that your data is in sync on all replicas and reduces the need for manual repairs. For operational simplicity and performance, Databases for DataStax enables the NodeSync service on all key spaces and tables to handle node repairs. You can also manually create tables with NodeSync enabled so that the service can repair the tables' data when necessary while the automation enables NodeSync . If a NodeSync-enabled table is targeted , manual nodetool repair or a repair operation from the OpsCenter node administration menu fails to run  .                           \n|user|: Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_10510-17837-19983",
      "text": "\nWorker nodes carry the deployments and services that make up your app. When you host workloads in the public cloud, you want to ensure that your app is protected from being accessed, changed, or monitored by an unauthorized user or software.\n\n\n\n Who owns the worker node and am I responsible to secure it? \n\nThe ownership of a worker node depends on the type of cluster that you create and the infrastructure provider that you choose.\n\n\n\n* Classic clusters: Worker nodes are provisioned in to your IBM Cloud account. The worker nodes are dedicated to you and you are responsible to request timely updates to the worker nodes to ensure that the worker node OS and IBM Cloud Kubernetes Service components apply the latest security updates and patches.\n* VPC clusters: Worker nodes are provisioned in to an IBM Cloud account that is owned by IBM to enable monitoring of malicious activities and apply security updates. You can't access your worker nodes by using the VPC dashboard. However, you can manage your worker nodes by using the IBM Cloud Kubernetes Service console, CLI, or API. The virtual machines that make up your worker nodes are dedicated to you and you are responsible to request timely updates so that your worker node OS and IBM Cloud Kubernetes Service components apply the latest security updates and patches.\n\n\n\nFor more information, see [Your responsibilities by using Red Hat OpenShift on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-responsibilities_iks).\n\nUse the ibmcloud oc worker update[command](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-clics_worker_update) regularly (such as monthly) to deploy updates and security patches to the operating system and to update the Red Hat OpenShift version that your worker nodes run. When updates are available, you are notified when you view information about the master and worker nodes in the IBM Cloud console or CLI, such as with the ibmcloud oc clusters ls or ibmcloud oc workers ls --cluster <cluster_name> commands. Worker node updates are provided by IBM as a full worker node image that includes the latest security patches.",
      "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-security",
      "score": 21.317507,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "yes",
            "timestamp": 1724674167
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_06079-10297-12436",
      "text": "\nWorker node With IBM Cloud Kubernetes Service, the virtual machines that your cluster manages are instances that are called worker nodes. These worker nodes virtual machines and all the worker node components are dedicated to you only and are not shared with other IBM customers. However, the underlying hardware is shared with other IBM customers. For more information, see [Virtual machines](https://cloud.ibm.com/docs/containers?topic=containers-planning_worker_nodesvm). You manage the worker nodes through the automation tools that are provided by IBM Cloud Kubernetes Service, such as the API, CLI, or console. Unlike classic clusters, you don't see VPC compute worker nodes in your infrastructure portal or separate infrastructure bill, but instead manage all maintenance and billing activity for the worker nodes from IBM Cloud Kubernetes Service. Worker nodes include the same [components](https://cloud.ibm.com/docs/containers?topic=containers-service-archworker-components) as described in the Classic architecture. Community Kubernetes worker nodes run on Ubuntu 18.04 x86_64, 16.04 x86_64 (deprecated). \n Cluster networking Your worker nodes are created in a VPC subnet in the zone that you specify. By default, the public and private cloud service endpoints for your cluster are enabled. Communication between the master and worker nodes is over the private network. Authenticated external users can communicate with the master over the public network, such as to run kubectl commands. You can optionally set up your cluster to communicate with on-prem services by setting up a VPC VPN on the private network. \n App networking You can create a Kubernetes LoadBalancer service for your apps in the cluster, which automatically provisions a VPC load balancer in your VPC outside the cluster. The load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. For more information, see [Exposing apps with VPC load balancers](https://cloud.ibm.com/docs/containers?topic=containers-vpc-lbaas). Calico is used as the cluster networking policy fabric.",
      "url": "https://cloud.ibm.com/docs/containers?topic=containers-service-arch",
      "score": 21.295753,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "yes",
            "timestamp": 1724673853
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_10488-8451-10309",
      "text": "\nWorker nodes <br><br> * Provide worker node patch operating system (OS), version, and security updates.<br> * Fulfill automation requests to update and recover worker nodes.<br><br><br> <br><br> * Use the API, CLI, or console tools to [apply](https://cloud.ibm.com/docs/containers?topic=containers-updateupdate) the provided worker node updates that include operating system patches; or to request that worker nodes are rebooted, reloaded, or replaced.<br><br><br> \n Cluster version <br><br> * Provide a suite of tools to automate cluster management, such as the Red Hat OpenShift on IBM Cloud [API](https://containers.cloud.ibm.com/global/swagger-global-api//), [CLI plug-in](https://cloud.ibm.com/docs/openshift?topic=openshift-kubernetes-service-cli), and [console](https://cloud.ibm.com/kubernetes/clusters).<br> * Automatically apply Red Hat OpenShift master patch OS, version, and security updates.<br> * Make major and minor updates for master nodes available for you to apply.<br> * Provide worker node major, minor, and patch OS, version, and security updates.<br> * Fulfill automation requests to update cluster master and worker nodes.<br><br><br> <br><br> * Use the API, CLI, or console tools to [apply](https://cloud.ibm.com/docs/containers?topic=containers-updateupdate) the provided major and minor Red Hat OpenShift master updates and major, minor, and patch worker node updates.<br><br><br> \n\n\n\n\n\n\n\n Identity and access management \n\nYou and IBM share responsibilities for controlling access to your Red Hat OpenShift on IBM Cloud instances. For IBM Cloud\u00ae Identity and Access Management responsibilities, consult that product's documentation. You are responsible for identity and access management to your application data.\n\n\n\nTable 4. Responsibilities for identity and access management\n\n Resource IBM responsibilities Your responsibilities",
      "url": "https://cloud.ibm.com/docs/openshift?topic=openshift-responsibilities_iks",
      "score": 20.858765,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "yes",
            "timestamp": 1724674397
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "Worker node"
                }
              }
            }
          }
        }
      }
    },
    {
      "document_id": "ibmcld_07578-398678-400724",
      "text": "\nFor more information, see [Updating the master](https://cloud.ibm.com/docs/containers?topic=containers-updatemaster).\n\nWorker nodes in standard clusters are provisioned in to your IBM Cloud infrastructure account. The worker nodes are dedicated to your account and you are responsible to request timely updates to the worker nodes to ensure that the worker node OS and Red Hat OpenShift on IBM Cloud components apply the latest security updates and patches. Security updates and patches are made available by IBM Site Reliability Engineers (SREs) who continuously monitor the Linux image that is installed on your worker nodes to detect vulnerabilities and security compliance issues. For more information, see [Updating worker nodes](https://cloud.ibm.com/docs/containers?topic=containers-updateworker_node).\n* Are the master and worker nodes highly available?\n\nThe Red Hat OpenShift on IBM Cloud architecture and infrastructure is designed to ensure reliability, low processing latency, and a maximum uptime of the service. By default, every cluster in Red Hat OpenShift on IBM Cloud is set up with multiple Red Hat OpenShift master instances to ensure availability and accessibility of your cluster resources, even if one or more instances of your Red Hat OpenShift master are unavailable.\n\nYou can make your cluster even more highly available and protect your app from a downtime by spreading your workloads across multiple worker nodes in multiple zones of a region. This setup is called a [multizone cluster](https://cloud.ibm.com/docs/openshift?topic=openshift-ha_clustersmz-clusters) and ensures that your app is accessible, even if a worker node or an entire zone is not available.\n\nTo protect against an entire region failure, create [multiple clusters and spread them across IBM Cloud regions](https://cloud.ibm.com/docs/openshift?topic=openshift-ha_clustersmultiple-clusters-glb). By setting up a network load balancer (NLB) for your clusters, you can achieve cross-region load balancing and cross-region networking for your clusters.",
      "url": "https://cloud.ibm.com/docs/faqs",
      "score": 20.194788,
      "feedback": {
        "relevant": {
          "868c6e58-57c1-48ee-8c67-832dbedcac18": {
            "value": "yes",
            "timestamp": 1724674341
          }
        }
      },
      "query": {
        "query": {
          "bool": {
            "must": {
              "text_expansion": {
                "ml.tokens": {
                  "model_id": ".elser_model_1",
                  "model_text": "Worker node"
                }
              }
            }
          }
        }
      }
    }
  ],
  "timestamp": 1724672458
}