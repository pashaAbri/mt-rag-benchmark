# Table 16c: Generation Results by Domain

<style>
table { color: #009900; font-weight: bold; }  /* Green bold for experimental results */
</style>

**Generated from experimental results**

Detailed generation results in the Reference (â€¢) retrieval setting using three metrics (RLF, RBllm, RBalg) broken down by domain.

**Note:** **Bold values** indicate the best-performing model for each metric-domain combination. <u>Underlined values</u> indicate the second-best performing model.

## Results by Domain

| | RLF | | | | | RBllm | | | | | RBalg | | | |
|-------|---------|------|------|-------|---|---------|------|------|-------|---|---------|------|------|-------|
| | CLAPNQ | FiQA | Govt | Cloud | | CLAPNQ | FiQA | Govt | Cloud | | CLAPNQ | FiQA | Govt | Cloud |
| **Command-R+ (104B)** | 0.73 | 0.70 | 0.72 | 0.61 |  | 0.67 | 0.64 | 0.63 | 0.56 |  | 0.44 | 0.36 | 0.42 | 0.39 |
| **GPT-4o** | 0.70 | 0.63 | 0.68 | 0.72 |  | 0.64 | 0.58 | 0.60 | 0.63 |  | 0.42 | 0.32 | 0.40 | 0.41 |
| **GPT-4o-mini** | 0.73 | 0.70 | 0.66 | 0.72 |  | 0.69 | 0.67 | 0.63 | 0.67 |  | 0.42 | 0.34 | 0.38 | 0.40 |
| **Llama 3.1 405B Instruct** | 0.75 | 0.77 | 0.78 | 0.77 |  | 0.75 | 0.73 | 0.77 | 0.71 |  | 0.48 | 0.39 | 0.48 | 0.48 |
| **Llama 3.1 70B Instruct** | 0.67 | 0.72 | 0.65 | 0.67 |  | 0.59 | 0.61 | 0.56 | 0.52 |  | 0.41 | 0.37 | 0.40 | 0.40 |
| **Llama 3.1 8B Instruct** | 0.57 | 0.51 | 0.51 | 0.51 |  | 0.46 | 0.40 | 0.42 | 0.40 |  | 0.35 | 0.28 | 0.32 | 0.33 |
| **Mixtral 8x22B Instruct** | 0.77 | 0.75 | 0.76 | 0.78 |  | 0.77 | 0.73 | 0.72 | 0.70 |  | 0.49 | 0.40 | 0.48 | 0.49 |
| **Qwen 2.5 (72B)** | 0.75 | 0.72 | 0.74 | 0.77 |  | 0.75 | 0.76 | 0.76 | 0.72 |  | 0.45 | 0.36 | 0.45 | 0.43 |
| **Qwen 2.5 (7B)** | 0.77 | 0.70 | 0.76 | 0.73 |  | 0.68 | 0.66 | 0.68 | 0.63 |  | 0.44 | 0.36 | 0.44 | 0.42 |
