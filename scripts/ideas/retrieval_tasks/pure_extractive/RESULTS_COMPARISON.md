# Pure Extractive vs Baseline Results Comparison

**Date:** November 9, 2024  
**Retrieval Method:** BM25  
**Evaluation Metrics:** NDCG@k and Recall@k (k=1,3,5,10)

## Executive Summary

This document compares the performance of **Pure Extractive** query rewriting against two baselines:
- **Lastturn**: Using only the current query without context
- **Human Rewrite**: Human-annotated query rewrites

## Overall Performance by Domain

### Performance Table (NDCG@10 / Recall@10)

| Domain | Lastturn | Human Rewrite | Pure Extractive | vs Lastturn | vs Human |
|--------|----------|---------------|-----------------|-------------|----------|
| **clapnq** | 0.269 / 0.361 | **0.301 / 0.399** | 0.290 / 0.389 | +7.8% / +7.8% | -3.7% / -2.5% |
| **cloud** | **0.252 / 0.320** | 0.248 / 0.327 | 0.239 / 0.309 | -5.2% / -3.4% | -3.6% / -5.5% |
| **fiqa** | 0.136 / 0.194 | **0.186 / 0.255** | 0.152 / 0.207 | +11.8% / +6.7% | -18.3% / -18.8% |
| **govt** | 0.319 / 0.402 | **0.354 / 0.452** | 0.339 / 0.418 | +6.3% / +4.0% | -4.2% / -7.5% |

**Bold** = Best performance per domain

## Detailed Metrics by Domain

### ClapNQ (Wikipedia Q&A) - 208 queries

| Method | NDCG@1 | NDCG@3 | NDCG@5 | NDCG@10 | Recall@1 | Recall@3 | Recall@5 | Recall@10 |
|--------|--------|--------|--------|---------|----------|----------|----------|-----------|
| Lastturn | 0.222 | 0.197 | 0.216 | 0.269 | 0.083 | 0.175 | 0.231 | 0.361 |
| Human Rewrite | **0.250** | **0.224** | **0.253** | **0.301** | **0.092** | **0.199** | **0.280** | **0.399** |
| Pure Extractive | 0.226 | 0.213 | 0.242 | 0.290 | 0.088 | 0.193 | 0.272 | 0.389 |

**Analysis:**
- ‚úÖ Pure Extractive outperforms Lastturn (+7.8% NDCG@10)
- ‚ö†Ô∏è Falls slightly short of Human Rewrite (-3.7% NDCG@10)
- üéØ Very competitive performance with simple keyword extraction

---

### Cloud (Technical Documentation) - 188 queries

| Method | NDCG@1 | NDCG@3 | NDCG@5 | NDCG@10 | Recall@1 | Recall@3 | Recall@5 | Recall@10 |
|--------|--------|--------|--------|---------|----------|----------|----------|-----------|
| Lastturn | **0.216** | **0.201** | **0.220** | **0.252** | **0.112** | **0.191** | **0.239** | **0.320** |
| Human Rewrite | 0.202 | 0.195 | 0.211 | 0.248 | 0.103 | 0.188 | 0.234 | 0.327 |
| Pure Extractive | 0.176 | 0.190 | 0.203 | 0.239 | 0.088 | 0.190 | 0.222 | 0.309 |

**Analysis:**
- ‚ùå Pure Extractive underperforms both baselines
- üìä Lastturn surprisingly performs best in technical domain
- üí≠ Technical queries may benefit from exact terminology over keyword extraction

---

### FiQA (Financial Q&A) - 180 queries

| Method | NDCG@1 | NDCG@3 | NDCG@5 | NDCG@10 | Recall@1 | Recall@3 | Recall@5 | Recall@10 |
|--------|--------|--------|--------|---------|----------|----------|----------|-----------|
| Lastturn | 0.083 | 0.094 | 0.111 | 0.136 | 0.032 | 0.095 | 0.133 | 0.194 |
| Human Rewrite | **0.139** | **0.131** | **0.156** | **0.186** | **0.057** | **0.125** | **0.183** | **0.255** |
| Pure Extractive | 0.089 | 0.113 | 0.127 | 0.152 | 0.033 | 0.118 | 0.151 | 0.207 |

**Analysis:**
- ‚úÖ Pure Extractive outperforms Lastturn (+11.8% NDCG@10)
- ‚ùå Falls short of Human Rewrite (-18.3% NDCG@10)
- üìà Financial domain shows most improvement over Lastturn
- üí° But human rewrites are significantly better, suggesting semantic understanding matters

---

### Govt (Government/Policy) - 201 queries

| Method | NDCG@1 | NDCG@3 | NDCG@5 | NDCG@10 | Recall@1 | Recall@3 | Recall@5 | Recall@10 |
|--------|--------|--------|--------|---------|----------|----------|----------|-----------|
| Lastturn | 0.268 | 0.253 | 0.282 | 0.319 | 0.117 | 0.238 | 0.312 | 0.402 |
| Human Rewrite | **0.289** | **0.280** | **0.305** | **0.354** | **0.125** | **0.271** | **0.338** | **0.452** |
| Pure Extractive | 0.269 | 0.283 | 0.304 | 0.339 | 0.120 | 0.275 | 0.335 | 0.418 |

**Analysis:**
- ‚úÖ Pure Extractive outperforms Lastturn (+6.3% NDCG@10)
- ‚ö†Ô∏è Close to Human Rewrite (-4.2% NDCG@10)
- üèÜ Best domain for Pure Extractive approach
- üìö Government domain benefits from comprehensive keyword coverage

---

## Key Findings

### Strengths of Pure Extractive

1. **Consistent Improvement over Lastturn**
   - 3 out of 4 domains show improvements
   - Average gain: +5.2% NDCG@10 (where it wins)

2. **Best Domains**
   - Government (0.339 NDCG@10) - close to human performance
   - FiQA (0.152 NDCG@10) - significant gain over lastturn

3. **High Recall**
   - Keyword-rich queries cast a wide net
   - Good recall@10 across all domains (21-42%)

### Weaknesses of Pure Extractive

1. **Domain Variability**
   - Cloud domain underperforms (-5.2% vs lastturn)
   - Inconsistent across different query types

2. **Gap from Human Performance**
   - Average -7.5% NDCG@10 behind human rewrites
   - Humans still provide better semantic understanding

3. **Repetition Issues**
   - Overlapping n-grams create keyword spam
   - May hurt precision despite helping recall

### Comparison: Pure Extractive vs Human Rewrite

**Pure Extractive wins when:**
- Keyword coverage matters more than semantics
- Government/policy queries with clear terminology
- Broad recall is more important than precision

**Human Rewrite wins when:**
- Semantic understanding is crucial
- Financial domain with complex concepts
- Precision at top ranks matters (NDCG@1, NDCG@3)

---

## Recommendations

### For Current Approach

1. **‚úÖ Keep Pure Extractive for:**
   - Government domain queries
   - Recall-focused applications
   - Low-resource scenarios (no human annotation)

2. **‚ùå Avoid Pure Extractive for:**
   - Cloud/technical documentation
   - Precision-critical applications

3. **üîß Potential Improvements:**
   - Deduplicate overlapping n-grams
   - Reduce max_terms from 10 to 5-7
   - Prefer longer n-grams over shorter ones
   - Test Hybrid Extractive approach

### Next Steps

1. **Test Hybrid Extractive**
   - Add templates + NER
   - Compare against these baselines

2. **Analyze Failure Cases**
   - Why does Cloud domain perform poorly?
   - What queries benefit most from human rewrites?

3. **Cross-Method Ensemble**
   - Combine Pure Extractive with other methods
   - Use Pure Extractive for recall, others for precision

---

## Methodology

### Query Rewriting Methods

**Lastturn:** Uses only the current user query without any conversation history.

**Human Rewrite:** Human annotators manually rewrite conversational queries into standalone queries, incorporating necessary context.

**Pure Extractive (Ours):** MMR-based algorithm that:
- Extracts unigrams, bigrams, and trigrams from query + history
- Selects top 10 terms using Maximal Marginal Relevance (Œª=0.7)
- Concatenates selected terms as keyword query

### Evaluation Setup

- **Retrieval:** BM25 (PyTerrier 0.13.1)
- **Corpus:** Passage-level documents per domain
- **Metrics:** NDCG@k and Recall@k (k=1,3,5,10)
- **Relevance Judgments:** Human annotations from MT-RAG benchmark

---

## Conclusion

Pure Extractive query rewriting demonstrates **mixed but promising results**:

‚úÖ **Strengths:**
- Outperforms Lastturn in 3/4 domains
- No human annotation required
- Simple, interpretable algorithm

‚ö†Ô∏è **Limitations:**
- Still 7.5% behind human rewrites on average
- Domain-dependent performance
- Keyword repetition may be suboptimal

üéØ **Overall Assessment:** Pure Extractive is a **viable low-resource alternative** to human rewrites, especially for government/policy domains. However, there's room for improvement through deduplication and hybrid approaches.

**Best Use Case:** When you need better-than-baseline performance without human annotation costs, especially for recall-focused applications in government/policy domains.

