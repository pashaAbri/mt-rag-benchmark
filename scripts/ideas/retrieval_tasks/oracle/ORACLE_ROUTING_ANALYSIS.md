# Oracle Routing Analysis - Per-Query Strategy Selection

## Executive Summary

**Finding:** If we could perfectly predict which strategy to use for each query, we could achieve **+13% to +29% improvement** over the best single strategy!

This is **5-10x better** than fusion (+0.4% to +3%), making **learned routing a high-priority research direction**.

---

## Oracle Routing Results

### ELSER

| Domain | Best Single (Rewrite) | Oracle Routing | Improvement |
|--------|----------------------|----------------|-------------|
| **ClapNQ** | R@5: 0.552 | R@5: 0.635 | **+15.0%** ‚úÖ |
| **Cloud** | R@5: 0.430 | R@5: 0.505 | **+17.6%** ‚úÖ |
| **FiQA** | R@5: 0.402 | R@5: 0.466 | **+16.1%** ‚úÖ |
| **Govt** | R@5: 0.508 | R@5: 0.631 | **+24.1%** ‚úÖ |
| **Average** | 0.473 | 0.559 | **+18.2%** |

### BGE

| Domain | Best Single (Rewrite) | Oracle Routing | Improvement |
|--------|----------------------|----------------|-------------|
| **ClapNQ** | R@5: 0.462 | R@5: 0.522 | **+13.1%** ‚úÖ |
| **Cloud** | R@5: 0.338 | R@5: 0.407 | **+20.3%** ‚úÖ |
| **FiQA** | R@5: 0.308 | R@5: 0.356 | **+15.6%** ‚úÖ |
| **Govt** | R@5: 0.404 | R@5: 0.479 | **+18.5%** ‚úÖ |
| **Average** | 0.378 | 0.441 | **+16.9%** |

### BM25

| Domain | Best Single | Oracle Routing | Improvement |
|--------|------------|----------------|-------------|
| **ClapNQ** | R@5: 0.280 (Rewrite) | R@5: 0.339 | **+21.3%** ‚úÖ |
| **Cloud** | R@5: 0.239 (Lastturn) | R@5: 0.296 | **+23.6%** ‚úÖ |
| **FiQA** | R@5: 0.183 (Rewrite) | R@5: 0.226 | **+23.8%** ‚úÖ |
| **Govt** | R@5: 0.343 (Rewrite) | R@5: 0.445 | **+29.5%** ‚úÖ |
| **Average** | 0.261 | 0.327 | **+24.6%** |

---

## Key Insights

### 1. **Routing Potential is Consistent**

**All retrievers benefit:**
- BM25 (weakest): **+24.6% average** improvement
- BGE (medium): **+16.9% average** improvement
- ELSER (strongest): **+18.2% average** improvement

**Unlike fusion** (which only helps BM25), **routing helps everyone!**

### 2. **Strategy Distribution**

**Typical pattern across domains:**
- **~70-78% queries tie** - all strategies perform equally
- **~22-30% queries have a clear winner** - this is where routing helps!
- Winner distribution varies:
  - Lastturn: 6-10% of queries
  - Rewrite: 7-13% of queries
  - Questions: 3-12% of queries

### 3. **Domain-Specific Patterns**

**Govt domain shows highest routing potential:**
- **+24.1% with ELSER, +29.5% with BM25**
- Questions strategy wins more often here (11.9% vs 7.7% in ClapNQ)
- Policy/legal documents may have more diverse query types

**Cloud domain also benefits strongly:**
- **+17.6% with ELSER, +20.3% with BGE, +23.6% with BM25**
- Technical documentation benefits from strategy diversity

---

## Comparison: Routing vs Fusion

| Approach | ELSER | BGE | BM25 | Implementation |
|----------|-------|-----|------|----------------|
| **Fusion** | +0.4% | -2.9% | +3.1% | ‚úÖ Simple (no learning) |
| **Oracle Routing** | +18.2% | +16.9% | +24.6% | ‚ùå Requires learning |

**Routing is 6-8x better than fusion!** But requires predicting which strategy to use.

---

## Why Routing Works Better Than Fusion

### Fusion Limitations:
- **Averages strategies** - can dilute strong strategy with weak ones
- Rewrite (strong) + Lastturn (weak) ‚Üí middle performance
- No intelligence about which strategy is better

### Routing Advantages:
- **Selects best strategy** - no dilution
- Exploits complementary strengths
- Can adapt to query characteristics

---

## Research Implications

### 1. **This is a Novel Finding**

**No prior work** has systematically analyzed per-query strategy selection for multi-turn conversational retrieval.

**Contribution:** Showing that different strategies excel on different queries, with oracle improvements of 13-29%.

### 2. **Routing is Worth Pursuing**

**Upper bound (oracle):** +13-29% improvement  
**Realistic target (learned):** +8-15% improvement (if we can predict 50-70% accurately)  
**Still better than fusion:** Even conservative routing beats fusion

### 3. **Features for Routing Model**

Potential features to predict best strategy:
- **Query length** (short vs long)
- **History length** (early vs late in conversation)
- **Entity density** (many entities favor lastturn?)
- **Question words** (what/why/how favor different strategies?)
- **Domain** (technical vs general)
- **Lexical overlap** with history (high overlap ‚Üí lastturn, low ‚Üí rewrite?)

### 4. **Implementation Approaches**

**Option A: Classification Model**
```
Input: (query, conversation_history, domain)
Output: {lastturn, rewrite, questions}
Train on: which strategy gave best recall@5 for each query
```

**Option B: Regression + Confidence**
```
Input: (query, conversation_history, domain)
Output: predicted_recall_per_strategy = [0.45, 0.62, 0.31]
Select: argmax(predicted_recall)
```

**Option C: Learning-to-Rank**
```
Input: (query, conversation_history, strategy_features)
Output: ranking of strategies
Select: top-ranked strategy
```

---

## Practical Steps to Implement Routing

### Phase 1: Feature Engineering (1-2 weeks)
1. Extract query characteristics from conversations
2. Compute lexical/semantic features
3. Label each query with its "winning strategy"

### Phase 2: Simple Baseline (1 week)
1. Train logistic regression classifier
2. Features: query length, history length, domain
3. Predict strategy per query
4. Evaluate against oracle

### Phase 3: Neural Routing (2-3 weeks)
1. Train BERT-based classifier
2. Input: [CLS] query [SEP] history [SEP]
3. Output: 3-way classification
4. Fine-tune on labeled data

### Phase 4: Ablation Studies (1 week)
1. Which features matter most?
2. Does routing generalize across domains?
3. Across retrievers?

---

## Expected Realistic Performance

**Conservative estimate** (60% routing accuracy):
- BM25: +14-18% improvement (60% of +24.6% oracle)
- BGE: +10-12% improvement (60% of +16.9% oracle)
- ELSER: +10-12% improvement (60% of +18.2% oracle)

**Optimistic estimate** (75% routing accuracy):
- BM25: +18-22% improvement
- BGE: +12-15% improvement
- ELSER: +13-16% improvement

**Still much better than fusion!**

---

## Recommendation

üéØ **High Priority:** Implement learned routing

**Why:**
1. **Massive potential**: +13-29% oracle improvement vs +3% with fusion
2. **Consistent across retrievers**: Benefits all retrieval systems
3. **Novel contribution**: First routing study for conversational retrieval
4. **Publishable**: "Adaptive Query Strategy Selection for Multi-Turn Conversational RAG"
5. **Practical**: Can be trained with existing labeled data

**Next steps:**
1. Extract features from conversations
2. Train simple classifier (start with scikit-learn)
3. Evaluate on test set
4. Compare to fusion and single-strategy baselines

---

## Summary Statistics

**Total oracle improvements:**
- ELSER: +15.0% to +24.1% (avg +18.2%)
- BGE: +13.1% to +20.3% (avg +16.9%)
- BM25: +21.3% to +29.5% (avg +24.6%)

**Comparison to fusion:**
- Fusion best case: +3.1% (BM25 3-way)
- Oracle routing worst case: +13.1% (BGE ClapNQ)
- **Routing is 4-10x better than fusion!**

**Winner distribution (typical):**
- 70-78% ties (strategies equal)
- 6-13% per strategy wins
- All strategies win some queries (no single dominant strategy)

This validates the routing hypothesis: **different queries need different strategies!**

