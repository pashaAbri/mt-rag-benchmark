# Incremental MMR Clustering & Query Rewriting Experiment

This experiment implements an **Incremental Multi-Turn RAG Pipeline** that simulates a real-world conversation flow. Unlike standard benchmarks that use pre-computed history, this pipeline generates its own context (agent responses) turn-by-turn.

## Pipeline Workflow

For each conversation, the system processes turns sequentially ($T_1 \rightarrow T_N$):

1.  **Context Building**:
    *   For Turn $N$, context is built from previous User Queries (from dataset) and **Generated Agent Responses** (from turns $1...N-1$).
2.  **Query Rewriting (MMR Clustering)**:
    *   Extracts sentences from the conversation history.
    *   Clusters sentences (using BGE embeddings) to identify topics.
    *   Selects diverse context using **Maximal Marginal Relevance (MMR)**.
    *   Uses LLM (Mixtral) to rewrite the current query into a standalone version using the selected context.
3.  **Retrieval (ELSER v2)**:
    *   Retrieves documents for the *rewritten query* using Elasticsearch (ELSER v2 model).
4.  **Generation**:
    *   Generates an answer using the retrieved documents and history.
    *   **Crucial Step**: This generated answer becomes the history for Turn $N+1$.

## Example Walkthrough (Conversation `3f5fa37...`)

**Turn 1**: "what makes the different shapes of the moon"
*   *No history yet.*
*   **Retrieve & Generate**: System answers about "lunar phases".

**Turn 2**: "What is the distance between the moon and earth?"
*   *Context*: Turn 1 Q + Turn 1 Generated Answer.
*   **Rewrite**: "Could you tell me the distance between the Earth and the Moon?"
*   **Retrieve & Generate**: System answers with average distance.

**Turn 5**: "what makes monkeys suitable for space travel?"
*   *Context*: Turns 1-4 Qs + Generated Answers.
*   **Rewrite**: "Why are monkeys suitable for space travel?" (Resolved reference).
*   **Retrieve**: Finds documents about primates in space.
*   **Generate**: Answer becomes context for Turn 6.

## Output Structure

The pipeline produces two main output types:

### 1. Rewritten Queries (`datasets/`)
JSONL file containing the final result of the rewriting process for each turn.
```json
{
  "_id": "task_id",
  "text": "|user|: Rewritten query text",
  "agent_response": "Generated answer text used for next turn's context"
}
```

### 2. Intermediate Data (`intermediate/`)
JSONL file containing detailed debugging and analysis information for every step of the pipeline.

**Key Fields:**
*   `original_query`: The user's raw query.
*   `rewritten_query`: The standalone query produced by the LLM.
*   `num_extracted_sentences`: Total sentences found in the conversation history.
*   `extracted_sentences`: List of all sentences from history.
*   `num_clusters`: Number of semantic clusters formed (based on history size).
*   `cluster_sizes`: Distribution of sentences across clusters.
*   `representative_sentences`: Sentences closest to the centroid of each cluster.
*   `selected_sentences`: Final sentences chosen by MMR for the rewriting context.
    *   `sentence`: Text of the sentence.
    *   `speaker`: Who said it (user/agent).
    *   `turn`: Which turn it came from.
    *   `cluster_id`: Which cluster it belongs to.
*   `contexts`: **Retrieved Documents** used for generation.
    *   `title`: Document title.
    *   `text`: Document content snippet.
    *   `score`: Retrieval relevance score (ELSER).
    *   `url`: Source URL.
*   `agent_response`: The answer generated by the LLM using the retrieved contexts.

## Usage

**Prerequisites**:
*   `ES_URL` and `ES_API_KEY` (Elasticsearch)
*   `TOGETHER_API_KEY` (LLM API)

**Run Full Experiment**:
```bash
python3 mmr_cluster_rewrite.py --domain clapnq --num-sentences 5
```

**Output**:
Results are saved to `datasets/` (rewritten queries) and `intermediate/` (pipeline stats).

