# Comprehensive Comparison: Query Rewriting Strategies

This document compares query rewriting strategies across two dimensions:

1. **Context source**: Ground-truth (human-repaired) responses vs LLM-generated responses
2. **Context filtering**: Full conversation history vs similarity-based filtering

## Key Distinction: Ground-Truth vs Live Pipeline

The paper's baseline uses **pre-computed rewrites** generated with access to **ground-truth (human-repaired) agent responses**. Our experiments use a **live pipeline** where agent responses are generated by an LLM and may contain errors that propagate to subsequent rewrites.

All experiments use **ELSER** (Learned Sparse) as the retrieval model.

## Experimental Conditions

| Strategy                 | LLM           | Context Source | Context Strategy | Description                             |
| ------------------------ | ------------- | -------------- | ---------------- | --------------------------------------- |
| **Mixtral - GT - Paper** | Mixtral 8x7B  | Ground-truth   | Full history     | Paper's reported results (Table 3)      |
| **Mixtral - GT**         | Mixtral 8x7B  | Ground-truth   | Full history     | Our evaluation of pre-computed rewrites |
| **Mixtral**              | Mixtral 8x7B  | LLM-generated  | Full history     | Live pipeline with Mixtral              |
| **Mixtral Filtered**     | Mixtral 8x7B  | LLM-generated  | Similarity ≥ 0.3 | Targeted rewrite with Mixtral           |
| **Sonnet**               | Claude Sonnet | LLM-generated  | Full history     | Live pipeline with Sonnet               |
| **Sonnet Filtered**      | Claude Sonnet | LLM-generated  | Similarity ≥ 0.3 | Targeted rewrite with Sonnet            |

---

## Results

| Domain | Strategy                 | R@1       | R@3       | R@5       | R@10      | nDCG@1    | nDCG@3    | nDCG@5    | nDCG@10   |
| ------ | ------------------------ | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| All    | **Mixtral - GT - Paper** | **0.200** | **0.430** | **0.520** | **0.640** | **0.460** | **0.450** | **0.480** | **0.540** |
|        | **Mixtral - GT**         | 0.187     | 0.372     | 0.476     | 0.608     | 0.429     | 0.399     | 0.438     | 0.495     |
|        | **Mixtral**              | 0.163     | 0.352     | 0.449     | 0.562     | 0.377     | 0.366     | 0.405     | 0.454     |
|        | **Mixtral Filtered**     | 0.164     | 0.352     | 0.449     | 0.566     | 0.384     | 0.370     | 0.407     | 0.458     |
|        | **Sonnet**               | 0.191     | 0.380     | 0.488     | 0.607     | 0.441     | 0.407     | 0.449     | 0.501     |
|        | **Sonnet Filtered**      | 0.187     | 0.380     | 0.483     | 0.602     | 0.440     | 0.405     | 0.444     | 0.495     |
|        |                          |           |           |           |           |           |           |           |           |
| ClapNQ | **Mixtral - GT - Paper** | 0.220     | 0.450     | 0.560     | 0.700     | 0.540     | 0.500     | 0.530     | 0.590     |
|        | **Mixtral - GT**         | 0.209     | 0.424     | 0.552     | 0.701     | 0.524     | 0.470     | 0.513     | 0.578     |
|        | **Mixtral**              | 0.198     | 0.445     | 0.558     | 0.693     | 0.500     | 0.475     | 0.514     | 0.574     |
|        | **Mixtral Filtered**     | 0.183     | 0.434     | 0.532     | 0.685     | 0.481     | 0.464     | 0.493     | 0.562     |
|        | **Sonnet**               | 0.220     | 0.445     | 0.580     | 0.726     | 0.548     | 0.493     | 0.539     | 0.604     |
|        | **Sonnet Filtered**      | **0.222** | **0.454** | **0.588** | **0.732** | **0.558** | **0.501** | **0.547** | **0.609** |
|        |                          |           |           |           |           |           |           |           |           |
| Cloud  | **Mixtral - GT - Paper** | **0.200** | **0.400** | **0.470** | **0.570** | **0.420** | **0.410** | **0.430** | **0.480** |
|        | **Mixtral - GT**         | 0.179     | 0.353     | 0.430     | 0.528     | 0.378     | 0.365     | 0.394     | 0.438     |
|        | **Mixtral**              | 0.127     | 0.267     | 0.332     | 0.435     | 0.287     | 0.273     | 0.300     | 0.344     |
|        | **Mixtral Filtered**     | 0.137     | 0.283     | 0.373     | 0.460     | 0.298     | 0.290     | 0.328     | 0.365     |
|        | **Sonnet**               | 0.160     | 0.321     | 0.414     | 0.505     | 0.362     | 0.334     | 0.375     | 0.414     |
|        | **Sonnet Filtered**      | 0.155     | 0.312     | 0.384     | 0.479     | 0.346     | 0.321     | 0.353     | 0.392     |
|        |                          |           |           |           |           |           |           |           |           |
| FiQA   | **Mixtral - GT - Paper** | **0.180** | **0.390** | **0.500** | **0.630** | **0.430** | **0.410** | **0.460** | **0.520** |
|        | **Mixtral - GT**         | 0.163     | 0.310     | 0.402     | 0.536     | 0.389     | 0.344     | 0.378     | 0.436     |
|        | **Mixtral**              | 0.149     | 0.278     | 0.362     | 0.480     | 0.322     | 0.296     | 0.332     | 0.385     |
|        | **Mixtral Filtered**     | 0.128     | 0.280     | 0.347     | 0.478     | 0.300     | 0.292     | 0.318     | 0.374     |
|        | **Sonnet**               | 0.175     | 0.324     | 0.414     | 0.532     | 0.411     | 0.355     | 0.391     | 0.443     |
|        | **Sonnet Filtered**      | 0.169     | 0.317     | 0.400     | 0.537     | 0.394     | 0.343     | 0.376     | 0.436     |
|        |                          |           |           |           |           |           |           |           |           |
| Govt   | **Mixtral - GT - Paper** | **0.210** | **0.470** | **0.560** | **0.670** | **0.470** | **0.480** | **0.510** | **0.560** |
|        | **Mixtral - GT**         | 0.194     | 0.392     | 0.508     | 0.651     | 0.413     | 0.407     | 0.454     | 0.517     |
|        | **Mixtral**              | 0.175     | 0.400     | 0.523     | 0.618     | 0.383     | 0.404     | 0.455     | 0.496     |
|        | **Mixtral Filtered**     | 0.202     | 0.396     | 0.525     | 0.620     | 0.438     | 0.417     | 0.470     | 0.511     |
|        | **Sonnet**               | 0.205     | 0.418     | 0.528     | 0.646     | 0.433     | 0.432     | 0.478     | 0.528     |
|        | **Sonnet Filtered**      | 0.199     | 0.426     | 0.541     | 0.643     | 0.448     | 0.439     | 0.484     | 0.528     |

---

## Context Filtering Statistics

**Note**: Statistics are for the **777 evaluable tasks** (tasks with relevance judgments).
The full pipeline processed 842 tasks, but 65 unanswerable queries have no qrels.

### Overview

| Metric                              | Sonnet Filtered | Mixtral Filtered |
| ----------------------------------- | --------------- | ---------------- |
| **Total evaluable tasks**           | 777             | 777              |
| **Turn 1 (no history)**             | 102             | 102              |
| **Multi-turn queries (Turn > 1)**   | 675             | 675              |
| **Average history turns available** | 3.98            | 3.98             |
| **Average turns SELECTED**          | 1.99            | 1.95             |
| **Average turns FILTERED OUT**      | 1.99            | 2.03             |
| **Average retention ratio**         | **59.6%**       | **59.1%**        |
| **Average filtering ratio**         | **40.4%**       | **40.9%**        |

The slight differences between Sonnet and Mixtral occur because filtering is based on semantic similarity between history turns and the LLM-generated response—different LLMs produce different responses, leading to different similarity scores.

## Key Findings

### Filtering Applied

| Metric                | Sonnet Filtered | Mixtral Filtered |
| --------------------- | --------------- | ---------------- |
| Some context filtered | 450 (66.7%)     | 442 (65.5%)      |
| All context kept      | 225 (33.3%)     | 233 (34.5%)      |
| All context filtered  | 0 (0%)          | 0 (0%)           |

Queries that kept all context are mostly Turn 2 queries with only 1 history turn, which is always included per `include_last_turn=True`. No queries had all context filtered because the last turn is always kept.

### Retention Rate Distribution

| Retention           | Sonnet Cases | Sonnet % | Mixtral Cases | Mixtral % |
| ------------------- | ------------ | -------- | ------------- | --------- |
| 100% (no filtering) | 225          | 33.3%    | 233           | 34.5%     |
| 75-99%              | 35           | 5.2%     | 30            | 4.4%      |
| 50-75%              | 147          | 21.8%    | 133           | 19.7%     |
| 25-50%              | 154          | 22.8%    | 155           | 23.0%     |
| 0-25%               | 114          | 16.9%    | 124           | 18.4%     |

### Filtering by Conversation Depth

As conversations get longer, more context gets filtered:

| History Depth | Cases | Sonnet Avg Sel | Sonnet Ret | Mixtral Avg Sel | Mixtral Ret |
| ------------- | ----- | -------------- | ---------- | --------------- | ----------- |
| 1 turn        | 106   | 1.0            | 100.0%     | 1.0             | 100.0%      |
| 2 turns       | 103   | 1.4            | 68.4%      | 1.4             | 69.9%       |
| 3 turns       | 99    | 1.7            | 56.2%      | 1.7             | 55.9%       |
| 4 turns       | 96    | 2.2            | 54.7%      | 2.1             | 52.1%       |
| 5 turns       | 87    | 2.6            | 52.9%      | 2.6             | 52.4%       |
| 6 turns       | 80    | 2.5            | 42.1%      | 2.5             | 42.3%       |
| 7 turns       | 65    | 2.7            | 38.7%      | 2.6             | 37.4%       |
| 8 turns       | 32    | 2.5            | 31.6%      | 2.3             | 28.5%       |

**Key insight**: On average, only ~2 turns are selected regardless of conversation length, suggesting the semantic similarity filtering is quite aggressive in keeping only the most relevant context. Both LLMs show similar filtering patterns, with Mixtral being slightly more aggressive (lower retention at deeper depths).

## Source

Data and code:

- MTRAG Paper Table 3 and Table 15 (Mixtral - GT - Paper)
- `scripts/baselines/retrieval_scripts/elser/results/elser_all_rewrite_evaluated_aggregate.csv` (Mixtral - GT)
- `scripts/ideas/retrieval_tasks/baseline_rewrite_with_mixtral/` (Mixtral)
- `scripts/ideas/retrieval_tasks/targeted_rewrite_with_mixtral/` (Mixtral Filtered)
- `scripts/ideas/retrieval_tasks/baseline_rewrite_with_sonnet/` (Sonnet)
- `scripts/ideas/retrieval_tasks/targeted_rewrite_with_sonnet/` (Sonnet Filtered)

Both Mixtral - GT baselines use pre-computed rewrites from `human/retrieval_tasks/*/rewrite.jsonl`, which were generated using Mixtral 8x7B with access to ground-truth (human-repaired) agent responses. The difference between "Paper" and our evaluation may be due to ElasticSearch version differences or evaluation implementation details.
