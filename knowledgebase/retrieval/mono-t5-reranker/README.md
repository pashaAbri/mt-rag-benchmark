# Oracle Retriever Analysis

## Overview

This directory contains an analysis exploring the **oracle retriever selection** scenario: *What if we could always pick the best retriever (BM25, BGE, or ELSER) for each query/turn?*

## Purpose

This analysis helps answer several key questions:

1. **How much improvement can we get from oracle selection?** - If we could perfectly choose the best retriever for each query, what would our performance be?

2. **Which retriever is chosen most often?** - Understanding which retriever performs best for different types of queries.

3. **Does oracle performance vary by domain?** - Are there domain-specific patterns in retriever performance?

4. **Is retriever selection/ensemble worth pursuing?** - If oracle provides significant gains, it suggests that learning to select or combine retrievers could be valuable.

## Methodology

For each query/turn (`task_id`), we:

1. **Load results** from all three retrievers (BM25, BGE, ELSER) for the same query
2. **Compare performance** using nDCG@5 as the primary selection metric
3. **Select the best retriever** for that query (the "oracle choice")
4. **Calculate aggregate performance** if we always used the oracle's choice
5. **Compare** oracle performance to individual retriever performance

## Files

- **`oracle_analysis_summary.md`** - Overview and links to strategy-specific reports
- **`oracle_analysis_lastturn.md`** - Analysis for "Last Turn" query strategy
- **`oracle_analysis_rewrite.md`** - Analysis for "Query Rewrite" query strategy  
- **`oracle_analysis_questions.md`** - Analysis for "Full Questions" query strategy

## Key Findings

### Query Rewrite Strategy (Most Common)

- **Oracle Improvement**: ~16% average improvement over best individual retriever
- **Most Chosen**: BM25 (39.3%), ELSER (39.6%), BGE (27.7%)
- **Primary Metric (nDCG@5)**: 17.49% improvement (0.4378 â†’ 0.5143)

### Domain Patterns

Different domains show different retriever preferences:
- **CLAPNQ**: ELSER chosen most often (44.2%)
- **FIQA**: ELSER chosen most often (43.9%)
- **GOVT**: BM25 chosen most often (38.8%)
- **CLOUD**: BM25 chosen most often (39.9%)

## Implications

The significant oracle gains (10-25% improvement) suggest that:

1. **Retriever selection is valuable** - Different retrievers excel at different types of queries
2. **Ensemble methods could help** - Combining retrievers might capture benefits of oracle selection
3. **Learning-based selection is promising** - A model that learns when to use which retriever could approach oracle performance

## Running the Analysis

To regenerate these reports:

```bash
python scripts/discovery/oracle_retriever_analysis.py
```

The script will:
- Load evaluated results from all three retrievers
- Calculate oracle performance for each strategy
- Generate markdown reports with detailed analysis

## Related Work

This analysis is related to:
- **Retriever Ensembles** - Combining multiple retrievers
- **Learned Retriever Selection** - Training models to choose retrievers
- **Query-Dependent Retrieval** - Adapting retrieval strategy to query characteristics

---

*Generated by `scripts/discovery/oracle_retriever_analysis.py`*
